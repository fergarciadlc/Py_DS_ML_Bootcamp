{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras Regression Code Along Project\n",
    "# Part 3: Creating a Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usual imports, and data import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "df = pd.read_csv('../DATA/kc_house_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop('id', axis=1)\n",
    "# date time\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "df['year'] = df['date'].apply(lambda date : date.year)\n",
    "df['month'] = df['date'].apply(lambda date : date.month)\n",
    "df = df.drop('date', axis=1)\n",
    "\n",
    "df = df.drop('zipcode',axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop('price', axis=1).values # values to have the numpy array\n",
    "y = df['price'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=101)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform scaling, after split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test) # We dont assume prior info from our test %set_env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We want to based our number of neurons (units) in our layer from the size of the actual feature data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15117, 19)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have 19 incoming features, probably to have a range of 19 neurons in our layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(19, activation='relu')) # Why 4 layers??\n",
    "model.add(Dense(19, activation='relu'))\n",
    "model.add(Dense(19, activation='relu'))\n",
    "model.add(Dense(19, activation='relu'))\n",
    "\n",
    "model.add(Dense(1)) #output predicted price\n",
    "\n",
    "model.compile(optimizer='adam',loss='mse')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation data: after each epoch we quickly run test data and check loss on test data\n",
    "to track how well performing model on train and test data, this will <b>NOT</b> adjust our weights or biases in our network\n",
    "\n",
    " * *validation_data*: Importat to have them in numpy arrays forms, nice plot to see if we're overfitting\n",
    "\n",
    "\n",
    "because is  large dataset, we'll fit in batches:\n",
    "\n",
    "\n",
    " * *batch_size*: usually powers of 2, the smaller the longer will take the training, but <b>less likely to overfit</b>! (because youre not passing entire dataset at once)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 15117 samples, validate on 6480 samples\n",
      "Epoch 1/400\n",
      "15117/15117 [==============================] - 6s 417us/sample - loss: 430240484408.9680 - val_loss: 418918654295.8618\n",
      "Epoch 2/400\n",
      "15117/15117 [==============================] - 1s 97us/sample - loss: 429165608152.5593 - val_loss: 415111405345.5012\n",
      "Epoch 3/400\n",
      "15117/15117 [==============================] - 2s 124us/sample - loss: 413150036138.1586 - val_loss: 377801060766.6568\n",
      "Epoch 4/400\n",
      "15117/15117 [==============================] - 1s 89us/sample - loss: 332204597788.2808 - val_loss: 247490837759.3679\n",
      "Epoch 5/400\n",
      "15117/15117 [==============================] - 2s 100us/sample - loss: 176851105679.1480 - val_loss: 110653639186.9630\n",
      "Epoch 6/400\n",
      "15117/15117 [==============================] - 1s 88us/sample - loss: 102570620489.4622 - val_loss: 95476297434.7062\n",
      "Epoch 7/400\n",
      "15117/15117 [==============================] - 1s 84us/sample - loss: 97810621257.5807 - val_loss: 93857973751.1506\n",
      "Epoch 8/400\n",
      "15117/15117 [==============================] - 1s 80us/sample - loss: 96063904297.2865 - val_loss: 92241728226.2914\n",
      "Epoch 9/400\n",
      "15117/15117 [==============================] - 1s 87us/sample - loss: 94298378049.2489 - val_loss: 90529396093.7877\n",
      "Epoch 10/400\n",
      "15117/15117 [==============================] - 1s 85us/sample - loss: 92495514295.1305 - val_loss: 88801046730.2716\n",
      "Epoch 11/400\n",
      "15117/15117 [==============================] - 1s 83us/sample - loss: 90635362011.7769 - val_loss: 86941292407.4667\n",
      "Epoch 12/400\n",
      "15117/15117 [==============================] - 1s 85us/sample - loss: 88774720219.5737 - val_loss: 85091557376.0000\n",
      "Epoch 13/400\n",
      "15117/15117 [==============================] - 1s 93us/sample - loss: 86742640205.2555 - val_loss: 83440093007.0123\n",
      "Epoch 14/400\n",
      "15117/15117 [==============================] - 2s 103us/sample - loss: 84734287004.4077 - val_loss: 81155310791.7432\n",
      "Epoch 15/400\n",
      "15117/15117 [==============================] - 2s 103us/sample - loss: 82555607587.3255 - val_loss: 79142788240.1185\n",
      "Epoch 16/400\n",
      "15117/15117 [==============================] - 1s 81us/sample - loss: 80340144361.8326 - val_loss: 76937602530.9235\n",
      "Epoch 17/400\n",
      "15117/15117 [==============================] - 1s 85us/sample - loss: 77984347098.2698 - val_loss: 74550713275.7333\n",
      "Epoch 18/400\n",
      "15117/15117 [==============================] - 1s 89us/sample - loss: 75568967229.1338 - val_loss: 72378061993.4025\n",
      "Epoch 19/400\n",
      "15117/15117 [==============================] - 2s 101us/sample - loss: 73112206236.6279 - val_loss: 69770928724.7012\n",
      "Epoch 20/400\n",
      "15117/15117 [==============================] - 1s 94us/sample - loss: 70563563355.1250 - val_loss: 67308453867.7728\n",
      "Epoch 21/400\n",
      "15117/15117 [==============================] - 1s 96us/sample - loss: 67992122103.4819 - val_loss: 64783410770.1728\n",
      "Epoch 22/400\n",
      "15117/15117 [==============================] - 2s 110us/sample - loss: 65400138673.6268 - val_loss: 62441260696.9679\n",
      "Epoch 23/400\n",
      "15117/15117 [==============================] - 2s 99us/sample - loss: 62896361896.0418 - val_loss: 60123488498.7259\n",
      "Epoch 24/400\n",
      "15117/15117 [==============================] - 2s 114us/sample - loss: 60597845454.9913 - val_loss: 57908003938.6074\n",
      "Epoch 25/400\n",
      "15117/15117 [==============================] - 2s 117us/sample - loss: 58488882372.3734 - val_loss: 55979829920.5531\n",
      "Epoch 26/400\n",
      "15117/15117 [==============================] - 1s 99us/sample - loss: 56579355298.6735 - val_loss: 54350924701.3926\n",
      "Epoch 27/400\n",
      "15117/15117 [==============================] - 2s 122us/sample - loss: 55019244173.2682 - val_loss: 52870521671.4272\n",
      "Epoch 28/400\n",
      "15117/15117 [==============================] - 2s 120us/sample - loss: 53671466635.8457 - val_loss: 51661773866.9827\n",
      "Epoch 29/400\n",
      "15117/15117 [==============================] - 2s 100us/sample - loss: 52502540561.8321 - val_loss: 50627627943.5062\n",
      "Epoch 30/400\n",
      "15117/15117 [==============================] - 2s 113us/sample - loss: 51514944677.0783 - val_loss: 49777420207.0914\n",
      "Epoch 31/400\n",
      "15117/15117 [==============================] - 1s 93us/sample - loss: 50620782640.9071 - val_loss: 49034525293.9852\n",
      "Epoch 32/400\n",
      "15117/15117 [==============================] - 1s 84us/sample - loss: 49867204837.0910 - val_loss: 48169279103.6840\n",
      "Epoch 33/400\n",
      "15117/15117 [==============================] - 1s 63us/sample - loss: 49105634873.0695 - val_loss: 47487804742.1630\n",
      "Epoch 34/400\n",
      "15117/15117 [==============================] - 1s 58us/sample - loss: 48403078680.8261 - val_loss: 46915701352.9284\n",
      "Epoch 35/400\n",
      "15117/15117 [==============================] - 1s 55us/sample - loss: 47785666970.6296 - val_loss: 46264909975.7037\n",
      "Epoch 36/400\n",
      "15117/15117 [==============================] - 2s 100us/sample - loss: 47180328023.4502 - val_loss: 45649781317.5309\n",
      "Epoch 37/400\n",
      "15117/15117 [==============================] - 2s 113us/sample - loss: 46620121530.3989 - val_loss: 45098436208.5136\n",
      "Epoch 38/400\n",
      "15117/15117 [==============================] - 2s 107us/sample - loss: 46072989653.8668 - val_loss: 44609805592.6519\n",
      "Epoch 39/400\n",
      "15117/15117 [==============================] - 2s 107us/sample - loss: 45604684182.7008 - val_loss: 44126976192.1580ss: 468453682\n",
      "Epoch 40/400\n",
      "15117/15117 [==============================] - 2s 105us/sample - loss: 45121519594.3237 - val_loss: 43713637590.9136\n",
      "Epoch 41/400\n",
      "15117/15117 [==============================] - 1s 95us/sample - loss: 44692400160.1757 - val_loss: 43241290087.0321\n",
      "Epoch 42/400\n",
      "15117/15117 [==============================] - 2s 109us/sample - loss: 44248758008.2948 - val_loss: 42867667550.8148\n",
      "Epoch 43/400\n",
      "15117/15117 [==============================] - 2s 107us/sample - loss: 43873722281.0240 - val_loss: 42485837743.0914\n",
      "Epoch 44/400\n",
      "15117/15117 [==============================] - 1s 94us/sample - loss: 43499424275.5425 - val_loss: 42118180621.2741\n",
      "Epoch 45/400\n",
      "15117/15117 [==============================] - 2s 110us/sample - loss: 43108656256.6350 - val_loss: 41914019774.2617\n",
      "Epoch 46/400\n",
      "15117/15117 [==============================] - 2s 101us/sample - loss: 42786334217.3818 - val_loss: 41458066788.5037\n",
      "Epoch 47/400\n",
      "15117/15117 [==============================] - 2s 110us/sample - loss: 42453025865.6315 - val_loss: 41182252418.8444\n",
      "Epoch 48/400\n",
      "15117/15117 [==============================] - 2s 107us/sample - loss: 42165797936.9748 - val_loss: 40938137888.2370\n",
      "Epoch 49/400\n",
      "15117/15117 [==============================] - 1s 98us/sample - loss: 41846749484.3517 - val_loss: 40527067065.2049\n",
      "Epoch 50/400\n",
      "15117/15117 [==============================] - 2s 103us/sample - loss: 41542698370.0406 - val_loss: 40241775249.3827\n",
      "Epoch 51/400\n",
      "15117/15117 [==============================] - 2s 113us/sample - loss: 41245940287.2337 - val_loss: 39922780873.0074\n",
      "Epoch 52/400\n",
      "15117/15117 [==============================] - 2s 110us/sample - loss: 40938106727.9952 - val_loss: 39648553045.9654\n",
      "Epoch 53/400\n",
      "15117/15117 [==============================] - 1s 98us/sample - loss: 40637652978.8588 - val_loss: 39366090620.5235\n",
      "Epoch 54/400\n",
      "15117/15117 [==============================] - 2s 107us/sample - loss: 40426452535.3083 - val_loss: 39066699558.5580\n",
      "Epoch 55/400\n",
      "15117/15117 [==============================] - 2s 106us/sample - loss: 40143247042.4090 - val_loss: 38807615037.9457\n",
      "Epoch 56/400\n",
      "15117/15117 [==============================] - 1s 82us/sample - loss: 39863939158.2986 - val_loss: 38579929431.8617\n",
      "Epoch 57/400\n",
      "15117/15117 [==============================] - 1s 68us/sample - loss: 39596380140.4914 - val_loss: 38317568591.6444\n",
      "Epoch 58/400\n",
      "15117/15117 [==============================] - 2s 110us/sample - loss: 39410742127.9884 - val_loss: 38118246546.6469\n",
      "Epoch 59/400\n",
      "15117/15117 [==============================] - 1s 96us/sample - loss: 39168313171.3351 - val_loss: 37818196362.4296\n",
      "Epoch 60/400\n",
      "15117/15117 [==============================] - 2s 103us/sample - loss: 38925720379.2541 - val_loss: 37600851007.2099\n",
      "Epoch 61/400\n",
      "15117/15117 [==============================] - 2s 110us/sample - loss: 38704056830.7468 - val_loss: 37383157942.0444\n",
      "Epoch 62/400\n",
      "15117/15117 [==============================] - 2s 103us/sample - loss: 38523148359.2607 - val_loss: 37184661266.3309\n",
      "Epoch 63/400\n",
      "15117/15117 [==============================] - 1s 79us/sample - loss: 38327819671.0395 - val_loss: 37014810037.4123\n",
      "Epoch 64/400\n",
      "15117/15117 [==============================] - 1s 67us/sample - loss: 38139129399.4438 - val_loss: 36921983461.4519\n",
      "Epoch 65/400\n",
      "15117/15117 [==============================] - 1s 60us/sample - loss: 38034357396.5501 - val_loss: 36674676437.6494\n",
      "Epoch 66/400\n",
      "15117/15117 [==============================] - 1s 98us/sample - loss: 37878574717.0788 - val_loss: 36524587994.0741\n",
      "Epoch 67/400\n",
      "15117/15117 [==============================] - 1s 95us/sample - loss: 37681423759.0464 - val_loss: 36422696034.6074\n",
      "Epoch 68/400\n",
      "15117/15117 [==============================] - 1s 98us/sample - loss: 37622607906.5465 - val_loss: 36250151068.7605\n",
      "Epoch 69/400\n",
      "15117/15117 [==============================] - 1s 88us/sample - loss: 37406358846.7087 - val_loss: 36112626291.0420\n",
      "Epoch 70/400\n",
      "15117/15117 [==============================] - 1s 75us/sample - loss: 37283647889.8914 - val_loss: 36041197896.6914\n",
      "Epoch 71/400\n",
      "15117/15117 [==============================] - 1s 93us/sample - loss: 37145693383.8957 - val_loss: 35883583078.4000\n",
      "Epoch 72/400\n",
      "15117/15117 [==============================] - 1s 95us/sample - loss: 37046710390.6775 - val_loss: 35752288711.1111\n",
      "Epoch 73/400\n",
      "15117/15117 [==============================] - 1s 89us/sample - loss: 36937749661.3561 - val_loss: 35720515199.6840\n",
      "Epoch 74/400\n",
      "15117/15117 [==============================] - 1s 89us/sample - loss: 36810433351.3454 - val_loss: 35609582028.1679\n",
      "Epoch 75/400\n",
      "15117/15117 [==============================] - 1s 97us/sample - loss: 36685138230.6479 - val_loss: 35422817899.4568\n",
      "Epoch 76/400\n",
      "15117/15117 [==============================] - 2s 103us/sample - loss: 36577746115.6960 - val_loss: 35376650846.8148\n",
      "Epoch 77/400\n",
      "15117/15117 [==============================] - 2s 102us/sample - loss: 36498792435.6039 - val_loss: 35232623264.5531\n",
      "Epoch 78/400\n",
      "15117/15117 [==============================] - 2s 109us/sample - loss: 36392523984.9049 - val_loss: 35113992283.0222\n",
      "Epoch 79/400\n",
      "15117/15117 [==============================] - 1s 96us/sample - loss: 36322227603.5171 - val_loss: 35130345032.0593\n",
      "Epoch 80/400\n",
      "15117/15117 [==============================] - 2s 102us/sample - loss: 36197816776.9626 - val_loss: 35103628260.1877\n",
      "Epoch 81/400\n",
      "15117/15117 [==============================] - 1s 98us/sample - loss: 36077967158.7495 - val_loss: 34826873019.1012\n",
      "Epoch 82/400\n",
      "15117/15117 [==============================] - 1s 99us/sample - loss: 36014578058.1015 - val_loss: 34739730796.0889\n",
      "Epoch 83/400\n",
      "15117/15117 [==============================] - 2s 108us/sample - loss: 35928237329.4595 - val_loss: 34655107785.0074\n",
      "Epoch 84/400\n",
      "15117/15117 [==============================] - 1s 91us/sample - loss: 35813229462.9379 - val_loss: 34607981191.2691\n",
      "Epoch 85/400\n",
      "15117/15117 [==============================] - 2s 101us/sample - loss: 35732049846.1652 - val_loss: 34497828515.0815\n",
      "Epoch 86/400\n",
      "15117/15117 [==============================] - 1s 97us/sample - loss: 35647604756.9311 - val_loss: 34405889183.2889\n",
      "Epoch 87/400\n",
      "15117/15117 [==============================] - 1s 89us/sample - loss: 35547475219.0175 - val_loss: 34284806073.2049\n",
      "Epoch 88/400\n",
      "15117/15117 [==============================] - 2s 108us/sample - loss: 35475953377.4670 - val_loss: 34212541103.7235\n",
      "Epoch 89/400\n",
      "15117/15117 [==============================] - 2s 111us/sample - loss: 35375326988.0066 - val_loss: 34145291893.5704\n",
      "Epoch 90/400\n",
      "15117/15117 [==============================] - 1s 89us/sample - loss: 35299946993.3347 - val_loss: 34058598582.0444\n",
      "Epoch 91/400\n",
      "15117/15117 [==============================] - 1s 97us/sample - loss: 35222755697.9866 - val_loss: 33948123318.0444\n",
      "Epoch 92/400\n",
      "15117/15117 [==============================] - 2s 106us/sample - loss: 35129872277.6509 - val_loss: 33896557823.3679\n",
      "Epoch 93/400\n",
      "15117/15117 [==============================] - 1s 95us/sample - loss: 35027741032.6387 - val_loss: 33766129613.4321\n",
      "Epoch 94/400\n",
      "15117/15117 [==============================] - 1s 82us/sample - loss: 34961792436.0992 - val_loss: 33675609500.1284\n",
      "Epoch 95/400\n",
      "15117/15117 [==============================] - 1s 87us/sample - loss: 34843970758.1346 - val_loss: 33612666379.3778\n",
      "Epoch 96/400\n",
      "15117/15117 [==============================] - 1s 94us/sample - loss: 34766236395.1535 - val_loss: 33509224604.7605\n",
      "Epoch 97/400\n",
      "15117/15117 [==============================] - 2s 107us/sample - loss: 34720671115.5240 - val_loss: 33464155128.4148\n",
      "Epoch 98/400\n",
      "15117/15117 [==============================] - 2s 101us/sample - loss: 34608881684.2199 - val_loss: 33418542876.4444\n",
      "Epoch 99/400\n",
      "15117/15117 [==============================] - 1s 93us/sample - loss: 34562091962.2973 - val_loss: 33298706300.5235\n",
      "Epoch 100/400\n",
      "15117/15117 [==============================] - 2s 101us/sample - loss: 34482313411.2218 - val_loss: 33234952788.7012\n",
      "Epoch 101/400\n",
      "15117/15117 [==============================] - 1s 94us/sample - loss: 34436352152.6821 - val_loss: 33265777798.0049\n",
      "Epoch 102/400\n",
      "15117/15117 [==============================] - 1s 96us/sample - loss: 34362006353.8448 - val_loss: 33123159442.0148\n",
      "Epoch 103/400\n",
      "15117/15117 [==============================] - 1s 93us/sample - loss: 34335151027.6589 - val_loss: 33056992668.1284\n",
      "Epoch 104/400\n",
      "15117/15117 [==============================] - 1s 96us/sample - loss: 34282057069.6514 - val_loss: 32996472381.9457\n",
      "Epoch 105/400\n",
      "15117/15117 [==============================] - 1s 84us/sample - loss: 34253647268.9258 - val_loss: 33006374669.2741\n",
      "Epoch 106/400\n",
      "15117/15117 [==============================] - 1s 81us/sample - loss: 34186974017.4521 - val_loss: 32884429588.8593\n",
      "Epoch 107/400\n",
      "15117/15117 [==============================] - 1s 98us/sample - loss: 34088370551.6767 - val_loss: 32843387593.0074\n",
      "Epoch 108/400\n",
      "15117/15117 [==============================] - 1s 89us/sample - loss: 34061771012.6570 - val_loss: 32775962947.6346\n",
      "Epoch 109/400\n",
      "15117/15117 [==============================] - 1s 83us/sample - loss: 34032715057.9739 - val_loss: 32700359303.2691\n",
      "Epoch 110/400\n",
      "15117/15117 [==============================] - 1s 75us/sample - loss: 33964306250.0549 - val_loss: 32653337137.3037\n",
      "Epoch 111/400\n",
      "15117/15117 [==============================] - 1s 95us/sample - loss: 33880070495.9682 - val_loss: 32631442813.7877\n",
      "Epoch 112/400\n",
      "15117/15117 [==============================] - 1s 99us/sample - loss: 33837094188.5549 - val_loss: 32538620308.5432\n",
      "Epoch 113/400\n",
      "15117/15117 [==============================] - 2s 104us/sample - loss: 33807498312.1413 - val_loss: 32488435792.9086\n",
      "Epoch 114/400\n",
      "15117/15117 [==============================] - 2s 105us/sample - loss: 33738423602.4142 - val_loss: 32448276035.0025\n",
      "Epoch 115/400\n",
      "15117/15117 [==============================] - 1s 94us/sample - loss: 33712884874.0507 - val_loss: 32446129452.8790\n",
      "Epoch 116/400\n",
      "15117/15117 [==============================] - 2s 106us/sample - loss: 33707381997.0163 - val_loss: 32313967886.5383\n",
      "Epoch 117/400\n",
      "15117/15117 [==============================] - 2s 105us/sample - loss: 33625151392.0826 - val_loss: 32282025028.2667\n",
      "Epoch 118/400\n",
      "15117/15117 [==============================] - 2s 104us/sample - loss: 33559242105.0314 - val_loss: 32374431076.5037\n",
      "Epoch 119/400\n",
      "15117/15117 [==============================] - 2s 104us/sample - loss: 33576076212.0654 - val_loss: 32219383206.2420\n",
      "Epoch 120/400\n",
      "15117/15117 [==============================] - 2s 105us/sample - loss: 33527861952.9526 - val_loss: 32184190733.2741\n",
      "Epoch 121/400\n",
      "15117/15117 [==============================] - 2s 105us/sample - loss: 33467886165.9938 - val_loss: 32183101369.2049\n",
      "Epoch 122/400\n",
      "15117/15117 [==============================] - 2s 106us/sample - loss: 33436433222.8712 - val_loss: 32027666715.1802\n",
      "Epoch 123/400\n",
      "15117/15117 [==============================] - 2s 104us/sample - loss: 33391209953.8226 - val_loss: 31993957686.9926\n",
      "Epoch 124/400\n",
      "15117/15117 [==============================] - 2s 101us/sample - loss: 33349879967.1850 - val_loss: 31947742776.8889\n",
      "Epoch 125/400\n",
      "15117/15117 [==============================] - 2s 103us/sample - loss: 33303194844.6237 - val_loss: 31910410174.2617\n",
      "Epoch 126/400\n",
      "15117/15117 [==============================] - 1s 97us/sample - loss: 33267473312.8277 - val_loss: 31856601993.1654\n",
      "Epoch 127/400\n",
      "15117/15117 [==============================] - 2s 103us/sample - loss: 33231813908.7110 - val_loss: 31941756022.8346\n",
      "Epoch 128/400\n",
      "15117/15117 [==============================] - 2s 104us/sample - loss: 33202985783.9688 - val_loss: 31767906192.7506\n",
      "Epoch 129/400\n",
      "15117/15117 [==============================] - 2s 104us/sample - loss: 33189955412.9608 - val_loss: 31730184940.4049\n",
      "Epoch 130/400\n",
      "15117/15117 [==============================] - 2s 103us/sample - loss: 33165480641.9686 - val_loss: 31693651876.9778\n",
      "Epoch 131/400\n",
      "15117/15117 [==============================] - 2s 102us/sample - loss: 33077644703.9809 - val_loss: 31669236490.7457\n",
      "Epoch 132/400\n",
      "15117/15117 [==============================] - 2s 103us/sample - loss: 33148038742.6034 - val_loss: 31623460009.4025\n",
      "Epoch 133/400\n",
      "15117/15117 [==============================] - 2s 104us/sample - loss: 33036316908.3390 - val_loss: 31594437300.7802\n",
      "Epoch 134/400\n",
      "15117/15117 [==============================] - 2s 105us/sample - loss: 33026556251.8362 - val_loss: 31522247980.8790\n",
      "Epoch 135/400\n",
      "15117/15117 [==============================] - 2s 104us/sample - loss: 32958216286.2579 - val_loss: 31488069980.9185\n",
      "Epoch 136/400\n",
      "15117/15117 [==============================] - 2s 105us/sample - loss: 32941094309.9419 - val_loss: 31446767701.9654\n",
      "Epoch 137/400\n",
      "15117/15117 [==============================] - 2s 104us/sample - loss: 32894238385.6437 - val_loss: 31407446425.6000\n",
      "Epoch 138/400\n",
      "15117/15117 [==============================] - 2s 103us/sample - loss: 32865756094.8357 - val_loss: 31376897815.3877\n",
      "Epoch 139/400\n",
      "15117/15117 [==============================] - 2s 103us/sample - loss: 32842729397.0814 - val_loss: 31397194471.3481\n",
      "Epoch 140/400\n",
      "15117/15117 [==============================] - 1s 91us/sample - loss: 32850022178.9022 - val_loss: 31302879075.2395\n",
      "Epoch 141/400\n",
      "15117/15117 [==============================] - 2s 104us/sample - loss: 32776317533.6143 - val_loss: 31265782806.7556\n",
      "Epoch 142/400\n",
      "15117/15117 [==============================] - 2s 104us/sample - loss: 32757909982.4018 - val_loss: 31261429537.5012\n",
      "Epoch 143/400\n",
      "15117/15117 [==============================] - 2s 104us/sample - loss: 32726618593.2129 - val_loss: 31223891876.9778\n",
      "Epoch 144/400\n",
      "15117/15117 [==============================] - 2s 104us/sample - loss: 32712390228.2326 - val_loss: 31145242138.5481\n",
      "Epoch 145/400\n",
      "15117/15117 [==============================] - 2s 104us/sample - loss: 32682656284.4162 - val_loss: 31189318112.3951\n",
      "Epoch 146/400\n",
      "15117/15117 [==============================] - 2s 103us/sample - loss: 32668652412.2490 - val_loss: 31107449150.5778\n",
      "Epoch 147/400\n",
      "15117/15117 [==============================] - 2s 104us/sample - loss: 32601385242.4687 - val_loss: 31022722126.3802\n",
      "Epoch 148/400\n",
      "15117/15117 [==============================] - 1s 99us/sample - loss: 32580040551.3179 - val_loss: 30997203035.0222\n",
      "Epoch 149/400\n",
      "15117/15117 [==============================] - 1s 80us/sample - loss: 32529006232.1741 - val_loss: 30983269924.6617\n",
      "Epoch 150/400\n",
      "15117/15117 [==============================] - 1s 72us/sample - loss: 32506496509.8662 - val_loss: 31005215463.3481\n",
      "Epoch 151/400\n",
      "15117/15117 [==============================] - 1s 64us/sample - loss: 32493689104.7144 - val_loss: 30914367144.1383\n",
      "Epoch 152/400\n",
      "15117/15117 [==============================] - 1s 94us/sample - loss: 32497481376.7091 - val_loss: 30948705479.7432\n",
      "Epoch 153/400\n",
      "15117/15117 [==============================] - 2s 103us/sample - loss: 32429847629.4249 - val_loss: 30853032532.7012\n",
      "Epoch 154/400\n",
      "15117/15117 [==============================] - 2s 104us/sample - loss: 32438861902.4410 - val_loss: 30908820174.0642\n",
      "Epoch 155/400\n",
      "15117/15117 [==============================] - 2s 105us/sample - loss: 32378289028.9195 - val_loss: 30913127527.6642\n",
      "Epoch 156/400\n",
      "15117/15117 [==============================] - 2s 103us/sample - loss: 32382904523.6214 - val_loss: 30759604835.8716\n",
      "Epoch 157/400\n",
      "15117/15117 [==============================] - 2s 103us/sample - loss: 32365264032.8446 - val_loss: 30795593439.7630\n",
      "Epoch 158/400\n",
      "15117/15117 [==============================] - 1s 84us/sample - loss: 32289116222.9966 - val_loss: 30823494845.6296\n",
      "Epoch 159/400\n",
      "15117/15117 [==============================] - 2s 104us/sample - loss: 32305353989.1312 - val_loss: 30700769446.8741\n",
      "Epoch 160/400\n",
      "15117/15117 [==============================] - 2s 103us/sample - loss: 32283090731.0308 - val_loss: 30632860644.1876\n",
      "Epoch 161/400\n",
      "15117/15117 [==============================] - 2s 104us/sample - loss: 32292996753.7390 - val_loss: 30644384305.3037\n",
      "Epoch 162/400\n",
      "15117/15117 [==============================] - 2s 104us/sample - loss: 32214672002.5656 - val_loss: 30598623459.5556\n",
      "Epoch 163/400\n",
      "15117/15117 [==============================] - 2s 104us/sample - loss: 32234053328.2614 - val_loss: 30609338691.6346\n",
      "Epoch 164/400\n",
      "15117/15117 [==============================] - 2s 102us/sample - loss: 32224261935.9079 - val_loss: 30542830771.5160\n",
      "Epoch 165/400\n",
      "15117/15117 [==============================] - 1s 88us/sample - loss: 32171273230.0218 - val_loss: 30524813438.4198\n",
      "Epoch 166/400\n",
      "15117/15117 [==============================] - 2s 104us/sample - loss: 32127231834.1089 - val_loss: 30514478153.3235\n",
      "Epoch 167/400\n",
      "15117/15117 [==============================] - 2s 104us/sample - loss: 32127810367.7587 - val_loss: 30484384558.1432\n",
      "Epoch 168/400\n",
      "15117/15117 [==============================] - 2s 104us/sample - loss: 32064889886.0081 - val_loss: 30502820279.9407\n",
      "Epoch 169/400\n",
      "15117/15117 [==============================] - 2s 104us/sample - loss: 32074777722.5386 - val_loss: 30396163726.8543\n",
      "Epoch 170/400\n",
      "15117/15117 [==============================] - 2s 103us/sample - loss: 32037166879.1088 - val_loss: 30413839585.0272\n",
      "Epoch 171/400\n",
      "15117/15117 [==============================] - 2s 103us/sample - loss: 32014071884.6120 - val_loss: 30364695837.7086\n",
      "Epoch 172/400\n",
      "15117/15117 [==============================] - 2s 103us/sample - loss: 31982921634.9953 - val_loss: 30313014340.2667\n",
      "Epoch 173/400\n",
      "15117/15117 [==============================] - 2s 101us/sample - loss: 31991833026.1888 - val_loss: 30288653931.4568\n",
      "Epoch 174/400\n",
      "15117/15117 [==============================] - 2s 104us/sample - loss: 31993878556.7210 - val_loss: 30303969219.3185\n",
      "Epoch 175/400\n",
      "15117/15117 [==============================] - 2s 105us/sample - loss: 31920871901.5551 - val_loss: 30244748209.6198\n",
      "Epoch 176/400\n",
      "15117/15117 [==============================] - 2s 104us/sample - loss: 31913486825.2061 - val_loss: 30215733961.0074\n",
      "Epoch 177/400\n",
      "15117/15117 [==============================] - 2s 104us/sample - loss: 31959145016.3244 - val_loss: 30179313042.0148\n",
      "Epoch 178/400\n",
      "15117/15117 [==============================] - 2s 102us/sample - loss: 31882703221.9155 - val_loss: 30217693037.3531\n",
      "Epoch 179/400\n",
      "15117/15117 [==============================] - 2s 102us/sample - loss: 31895020975.8317 - val_loss: 30146608975.0123\n",
      "Epoch 180/400\n",
      "15117/15117 [==============================] - 2s 105us/sample - loss: 31829744498.9011 - val_loss: 30104670390.0444\n",
      "Epoch 181/400\n",
      "15117/15117 [==============================] - 2s 106us/sample - loss: 31846995298.8810 - val_loss: 30111977340.5235\n",
      "Epoch 182/400\n",
      "15117/15117 [==============================] - 2s 104us/sample - loss: 31794734552.0683 - val_loss: 30038987368.9284\n",
      "Epoch 183/400\n",
      "15117/15117 [==============================] - 1s 90us/sample - loss: 31804260276.8105 - val_loss: 30062376034.6074\n",
      "Epoch 184/400\n",
      "15117/15117 [==============================] - 2s 103us/sample - loss: 31774053772.7771 - val_loss: 29989583085.6691\n",
      "Epoch 185/400\n",
      "15117/15117 [==============================] - 2s 102us/sample - loss: 31718084380.8735 - val_loss: 30086651590.4790\n",
      "Epoch 186/400\n",
      "15117/15117 [==============================] - 2s 103us/sample - loss: 31735906229.0137 - val_loss: 30034521495.0716\n",
      "Epoch 187/400\n",
      "15117/15117 [==============================] - 2s 103us/sample - loss: 31740192662.3960 - val_loss: 29938559926.6765\n",
      "Epoch 188/400\n",
      "15117/15117 [==============================] - 2s 104us/sample - loss: 31679896596.9311 - val_loss: 29922772066.6074\n",
      "Epoch 189/400\n",
      "15117/15117 [==============================] - 2s 104us/sample - loss: 31698370050.0660 - val_loss: 29930592918.4395\n",
      "Epoch 190/400\n",
      "15117/15117 [==============================] - 1s 98us/sample - loss: 31668163809.3653 - val_loss: 30189019247.2494\n",
      "Epoch 191/400\n",
      "15117/15117 [==============================] - 2s 105us/sample - loss: 31732024788.4104 - val_loss: 29906843076.5827\n",
      "Epoch 192/400\n",
      "15117/15117 [==============================] - 2s 101us/sample - loss: 31608331547.7558 - val_loss: 29854677967.9605\n",
      "Epoch 193/400\n",
      "15117/15117 [==============================] - 1s 79us/sample - loss: 31582493943.2448 - val_loss: 29846941321.7975\n",
      "Epoch 194/400\n",
      "15117/15117 [==============================] - 1s 65us/sample - loss: 31593611136.9229 - val_loss: 29773800981.4914\n",
      "Epoch 195/400\n",
      "15117/15117 [==============================] - 1s 61us/sample - loss: 31542645469.6059 - val_loss: 29763029320.6914\n",
      "Epoch 196/400\n",
      "15117/15117 [==============================] - 1s 57us/sample - loss: 31559770440.1244 - val_loss: 29765748559.0123\n",
      "Epoch 197/400\n",
      "15117/15117 [==============================] - 1s 62us/sample - loss: 31559956673.3251 - val_loss: 29849708442.8642\n",
      "Epoch 198/400\n",
      "15117/15117 [==============================] - 1s 59us/sample - loss: 31523879419.9018 - val_loss: 29820261631.3679\n",
      "Epoch 199/400\n",
      "15117/15117 [==============================] - 1s 54us/sample - loss: 31486181129.7035 - val_loss: 29696175192.4938\n",
      "Epoch 200/400\n",
      "15117/15117 [==============================] - 1s 53us/sample - loss: 31490551336.4059 - val_loss: 29697357424.5136\n",
      "Epoch 201/400\n",
      "15117/15117 [==============================] - 1s 54us/sample - loss: 31470533215.6126 - val_loss: 29679574964.1481\n",
      "Epoch 202/400\n",
      "15117/15117 [==============================] - 1s 54us/sample - loss: 31439203979.7780 - val_loss: 29609415849.4025\n",
      "Epoch 203/400\n",
      "15117/15117 [==============================] - 1s 53us/sample - loss: 31448688224.7642 - val_loss: 29682896734.1827\n",
      "Epoch 204/400\n",
      "15117/15117 [==============================] - 1s 54us/sample - loss: 31369742062.7437 - val_loss: 29712948272.0395\n",
      "Epoch 205/400\n",
      "15117/15117 [==============================] - 1s 52us/sample - loss: 31407505542.6638 - val_loss: 29649595971.0025\n",
      "Epoch 206/400\n",
      "15117/15117 [==============================] - 1s 54us/sample - loss: 31384170437.5418 - val_loss: 29558204815.4864\n",
      "Epoch 207/400\n",
      "15117/15117 [==============================] - 1s 54us/sample - loss: 31329479825.4341 - val_loss: 29986111189.6494\n",
      "Epoch 208/400\n",
      "15117/15117 [==============================] - 1s 54us/sample - loss: 31392203475.4451 - val_loss: 29554836664.5728\n",
      "Epoch 209/400\n",
      "15117/15117 [==============================] - 1s 54us/sample - loss: 31312879480.1508 - val_loss: 29501352649.0074\n",
      "Epoch 210/400\n",
      "15117/15117 [==============================] - 1s 49us/sample - loss: 31298640206.3902 - val_loss: 29712056679.0321\n",
      "Epoch 211/400\n",
      "15117/15117 [==============================] - 1s 50us/sample - loss: 31301793647.8529 - val_loss: 29456371656.3753\n",
      "Epoch 212/400\n",
      "15117/15117 [==============================] - 1s 50us/sample - loss: 31308915592.4419 - val_loss: 29433697762.9235\n",
      "Epoch 213/400\n",
      "15117/15117 [==============================] - 1s 53us/sample - loss: 31277916859.8722 - val_loss: 29456543359.6840\n",
      "Epoch 214/400\n",
      "15117/15117 [==============================] - 1s 49us/sample - loss: 31236503502.4156 - val_loss: 29407222887.6642\n",
      "Epoch 215/400\n",
      "15117/15117 [==============================] - 1s 50us/sample - loss: 31225325353.2696 - val_loss: 29403659524.4247\n",
      "Epoch 216/400\n",
      "15117/15117 [==============================] - 1s 52us/sample - loss: 31224924739.4335 - val_loss: 29381171968.6321\n",
      "Epoch 217/400\n",
      "15117/15117 [==============================] - 1s 53us/sample - loss: 31195173350.5643 - val_loss: 29362541970.0148\n",
      "Epoch 218/400\n",
      "15117/15117 [==============================] - 1s 53us/sample - loss: 31182948578.9233 - val_loss: 29322089742.5383\n",
      "Epoch 219/400\n",
      "15117/15117 [==============================] - 1s 54us/sample - loss: 31155783507.5383 - val_loss: 29295864318.7358\n",
      "Epoch 220/400\n",
      "15117/15117 [==============================] - 1s 54us/sample - loss: 31183337811.5721 - val_loss: 29284867605.4914\n",
      "Epoch 221/400\n",
      "15117/15117 [==============================] - 1s 52us/sample - loss: 31176509844.9904 - val_loss: 29290117661.0765\n",
      "Epoch 222/400\n",
      "15117/15117 [==============================] - 1s 52us/sample - loss: 31141869759.4285 - val_loss: 29259474999.6247\n",
      "Epoch 223/400\n",
      "15117/15117 [==============================] - 1s 55us/sample - loss: 31120677305.9925 - val_loss: 29239062280.2173\n",
      "Epoch 224/400\n",
      "15117/15117 [==============================] - 1s 53us/sample - loss: 31156785124.6337 - val_loss: 29250844813.5901\n",
      "Epoch 225/400\n",
      "15117/15117 [==============================] - 1s 51us/sample - loss: 31099625040.1005 - val_loss: 29209844784.0395\n",
      "Epoch 226/400\n",
      "15117/15117 [==============================] - 1s 54us/sample - loss: 31107851763.2991 - val_loss: 29204062344.5333\n",
      "Epoch 227/400\n",
      "15117/15117 [==============================] - 1s 53us/sample - loss: 31063221526.7431 - val_loss: 29272955800.3358\n",
      "Epoch 228/400\n",
      "15117/15117 [==============================] - 1s 50us/sample - loss: 31070662142.5436 - val_loss: 29177036648.2963\n",
      "Epoch 229/400\n",
      "15117/15117 [==============================] - 1s 52us/sample - loss: 31044915715.8950 - val_loss: 29241395473.0667\n",
      "Epoch 230/400\n",
      "15117/15117 [==============================] - 1s 50us/sample - loss: 31023962167.5454 - val_loss: 29167960544.3951\n",
      "Epoch 231/400\n",
      "15117/15117 [==============================] - 1s 46us/sample - loss: 31030651110.5812 - val_loss: 29109182362.8642\n",
      "Epoch 232/400\n",
      "15117/15117 [==============================] - 1s 46us/sample - loss: 31007015370.1142 - val_loss: 29089605535.9210\n",
      "Epoch 233/400\n",
      "15117/15117 [==============================] - 1s 47us/sample - loss: 30982210585.2664 - val_loss: 29104150947.7136\n",
      "Epoch 234/400\n",
      "15117/15117 [==============================] - 1s 46us/sample - loss: 30981331239.7455 - val_loss: 29125204342.2025\n",
      "Epoch 235/400\n",
      "15117/15117 [==============================] - 1s 47us/sample - loss: 30989666516.5628 - val_loss: 29060972872.6914\n",
      "Epoch 236/400\n",
      "15117/15117 [==============================] - 1s 47us/sample - loss: 30960917988.0579 - val_loss: 29068116969.2444\n",
      "Epoch 237/400\n",
      "15117/15117 [==============================] - 1s 47us/sample - loss: 30937836222.4463 - val_loss: 29009857184.5531\n",
      "Epoch 238/400\n",
      "15117/15117 [==============================] - 1s 46us/sample - loss: 30938349542.8013 - val_loss: 29035827933.2346\n",
      "Epoch 239/400\n",
      "15117/15117 [==============================] - 1s 47us/sample - loss: 30920809671.2184 - val_loss: 28981946110.1037\n",
      "Epoch 240/400\n",
      "15117/15117 [==============================] - 1s 46us/sample - loss: 30929301439.5809 - val_loss: 29095101093.6099\n",
      "Epoch 241/400\n",
      "15117/15117 [==============================] - 1s 47us/sample - loss: 30935063909.4551 - val_loss: 29000716824.0198\n",
      "Epoch 242/400\n",
      "15117/15117 [==============================] - 1s 92us/sample - loss: 30921444265.0240 - val_loss: 28951756987.1012\n",
      "Epoch 243/400\n",
      "15117/15117 [==============================] - 1s 91us/sample - loss: 30912539336.3360 - val_loss: 28946198608.9086\n",
      "Epoch 244/400\n",
      "15117/15117 [==============================] - 1s 90us/sample - loss: 30875481613.8525 - val_loss: 28923598499.0815\n",
      "Epoch 245/400\n",
      "15117/15117 [==============================] - 1s 90us/sample - loss: 30863913502.5838 - val_loss: 28945060654.1432\n",
      "Epoch 246/400\n",
      "15117/15117 [==============================] - 1s 91us/sample - loss: 30865275076.2717 - val_loss: 29027462510.6173\n",
      "Epoch 247/400\n",
      "15117/15117 [==============================] - 1s 74us/sample - loss: 30886288620.0680 - val_loss: 28873253420.2469\n",
      "Epoch 248/400\n",
      "15117/15117 [==============================] - 1s 74us/sample - loss: 30872681089.7527 - val_loss: 28973452538.3111\n",
      "Epoch 249/400\n",
      "15117/15117 [==============================] - 1s 74us/sample - loss: 30818625490.3444 - val_loss: 28882787436.7210\n",
      "Epoch 250/400\n",
      "15117/15117 [==============================] - 1s 73us/sample - loss: 30761856156.5432 - val_loss: 28910212533.4123\n",
      "Epoch 251/400\n",
      "15117/15117 [==============================] - 1s 74us/sample - loss: 30798927405.6895 - val_loss: 28830667202.0543\n",
      "Epoch 252/400\n",
      "15117/15117 [==============================] - 1s 74us/sample - loss: 30782206074.7418 - val_loss: 28845759546.1531\n",
      "Epoch 253/400\n",
      "15117/15117 [==============================] - 1s 73us/sample - loss: 30802638749.2375 - val_loss: 28821546310.1630\n",
      "Epoch 254/400\n",
      "15117/15117 [==============================] - 1s 74us/sample - loss: 30758864800.7599 - val_loss: 28780435825.1457\n",
      "Epoch 255/400\n",
      "15117/15117 [==============================] - 1s 74us/sample - loss: 30780973516.6205 - val_loss: 28757509724.2864\n",
      "Epoch 256/400\n",
      "15117/15117 [==============================] - 1s 74us/sample - loss: 30742535568.6721 - val_loss: 28771903680.1580\n",
      "Epoch 257/400\n",
      "15117/15117 [==============================] - 1s 74us/sample - loss: 30722842012.1876 - val_loss: 28733565236.4642\n",
      "Epoch 258/400\n",
      "15117/15117 [==============================] - 1s 73us/sample - loss: 30725581143.1623 - val_loss: 28735889165.2741\n",
      "Epoch 259/400\n",
      "15117/15117 [==============================] - 1s 74us/sample - loss: 30711150089.2801 - val_loss: 28689818019.7136\n",
      "Epoch 260/400\n",
      "15117/15117 [==============================] - 1s 74us/sample - loss: 30699683915.5960 - val_loss: 28712304589.4321\n",
      "Epoch 261/400\n",
      "15117/15117 [==============================] - 1s 72us/sample - loss: 30699879063.9709 - val_loss: 28672763317.4123\n",
      "Epoch 262/400\n",
      "15117/15117 [==============================] - 1s 65us/sample - loss: 30685175822.5637 - val_loss: 28787349549.5111\n",
      "Epoch 263/400\n",
      "15117/15117 [==============================] - 1s 64us/sample - loss: 30694538412.1230 - val_loss: 28677566974.7358\n",
      "Epoch 264/400\n",
      "15117/15117 [==============================] - 1s 65us/sample - loss: 30642348459.7674 - val_loss: 28746013157.4519\n",
      "Epoch 265/400\n",
      "15117/15117 [==============================] - 1s 65us/sample - loss: 30647009283.3869 - val_loss: 28695520607.4469\n",
      "Epoch 266/400\n",
      "15117/15117 [==============================] - 1s 66us/sample - loss: 30659470670.2885 - val_loss: 28658109922.9235\n",
      "Epoch 267/400\n",
      "15117/15117 [==============================] - 1s 64us/sample - loss: 30633604555.1980 - val_loss: 28670860836.6617\n",
      "Epoch 268/400\n",
      "15117/15117 [==============================] - 1s 59us/sample - loss: 30603104579.5182 - val_loss: 28620838669.2741\n",
      "Epoch 269/400\n",
      "15117/15117 [==============================] - 1s 56us/sample - loss: 30603832052.6369 - val_loss: 28632227445.5704\n",
      "Epoch 270/400\n",
      "15117/15117 [==============================] - 1s 53us/sample - loss: 30630889800.5308 - val_loss: 28646396450.1333\n",
      "Epoch 271/400\n",
      "15117/15117 [==============================] - 1s 53us/sample - loss: 30627584679.7539 - val_loss: 28608778882.2123\n",
      "Epoch 272/400\n",
      "15117/15117 [==============================] - 1s 52us/sample - loss: 30569901245.5657 - val_loss: 28627494360.8099\n",
      "Epoch 273/400\n",
      "15117/15117 [==============================] - 1s 52us/sample - loss: 30561829475.3382 - val_loss: 28594319461.1358\n",
      "Epoch 274/400\n",
      "15117/15117 [==============================] - 1s 53us/sample - loss: 30616906209.2129 - val_loss: 28563238482.1728\n",
      "Epoch 275/400\n",
      "15117/15117 [==============================] - 1s 53us/sample - loss: 30577910922.7958 - val_loss: 28529592972.3259\n",
      "Epoch 276/400\n",
      "15117/15117 [==============================] - 1s 52us/sample - loss: 30552085642.9313 - val_loss: 28552084335.8815\n",
      "Epoch 277/400\n",
      "15117/15117 [==============================] - 1s 53us/sample - loss: 30566407730.9053 - val_loss: 28578469089.0272\n",
      "Epoch 278/400\n",
      "15117/15117 [==============================] - 1s 53us/sample - loss: 30509338992.9028 - val_loss: 28517923250.8839\n",
      "Epoch 279/400\n",
      "15117/15117 [==============================] - 1s 53us/sample - loss: 30502351791.4592 - val_loss: 28474113259.1407\n",
      "Epoch 280/400\n",
      "15117/15117 [==============================] - 1s 52us/sample - loss: 30523374044.5051 - val_loss: 28516183315.5951\n",
      "Epoch 281/400\n",
      "15117/15117 [==============================] - 1s 53us/sample - loss: 30515151314.4460 - val_loss: 28597884240.2765\n",
      "Epoch 282/400\n",
      "15117/15117 [==============================] - 1s 53us/sample - loss: 30478739268.7713 - val_loss: 28442471659.1407\n",
      "Epoch 283/400\n",
      "15117/15117 [==============================] - 1s 52us/sample - loss: 30454543283.5235 - val_loss: 28474870892.7210\n",
      "Epoch 284/400\n",
      "15117/15117 [==============================] - 1s 53us/sample - loss: 30478548733.1719 - val_loss: 28457299057.7778\n",
      "Epoch 285/400\n",
      "15117/15117 [==============================] - 1s 54us/sample - loss: 30447732599.3041 - val_loss: 28431011334.3210\n",
      "Epoch 286/400\n",
      "15117/15117 [==============================] - 1s 54us/sample - loss: 30435611992.8557 - val_loss: 28410227067.2593\n",
      "Epoch 287/400\n",
      "15117/15117 [==============================] - 1s 53us/sample - loss: 30442312362.3957 - val_loss: 28436008995.3975\n",
      "Epoch 288/400\n",
      "15117/15117 [==============================] - 1s 54us/sample - loss: 30442549701.9821 - val_loss: 28377798731.8519\n",
      "Epoch 289/400\n",
      "15117/15117 [==============================] - 1s 54us/sample - loss: 30450024120.6885 - val_loss: 28378237732.0296\n",
      "Epoch 290/400\n",
      "15117/15117 [==============================] - 1s 54us/sample - loss: 30402870167.4798 - val_loss: 28337936368.8296\n",
      "Epoch 291/400\n",
      "15117/15117 [==============================] - 1s 54us/sample - loss: 30386350791.8280 - val_loss: 28330359486.8938\n",
      "Epoch 292/400\n",
      "15117/15117 [==============================] - 1s 91us/sample - loss: 30435643059.8113 - val_loss: 28363116564.2272\n",
      "Epoch 293/400\n",
      "15117/15117 [==============================] - 1s 90us/sample - loss: 30390455471.1035 - val_loss: 28366710849.7383\n",
      "Epoch 294/400\n",
      "15117/15117 [==============================] - 1s 89us/sample - loss: 30405520290.7243 - val_loss: 28370379333.5309\n",
      "Epoch 295/400\n",
      "15117/15117 [==============================] - 1s 91us/sample - loss: 30374063297.5283 - val_loss: 28301631311.0123\n",
      "Epoch 296/400\n",
      "15117/15117 [==============================] - 1s 89us/sample - loss: 30361463369.2251 - val_loss: 28285805342.9728\n",
      "Epoch 297/400\n",
      "15117/15117 [==============================] - 1s 90us/sample - loss: 30393201961.5744 - val_loss: 28283943852.5630\n",
      "Epoch 298/400\n",
      "15117/15117 [==============================] - 1s 89us/sample - loss: 30351688522.2920 - val_loss: 28261430274.5284\n",
      "Epoch 299/400\n",
      "15117/15117 [==============================] - 1s 90us/sample - loss: 30364084655.8995 - val_loss: 28269048515.9506\n",
      "Epoch 300/400\n",
      "15117/15117 [==============================] - 1s 91us/sample - loss: 30347432604.1707 - val_loss: 28261667382.3605\n",
      "Epoch 301/400\n",
      "15117/15117 [==============================] - 1s 88us/sample - loss: 30289034458.1851 - val_loss: 28271571060.3062\n",
      "Epoch 302/400\n",
      "15117/15117 [==============================] - 1s 93us/sample - loss: 30272845201.7559 - val_loss: 28208943189.9654\n",
      "Epoch 303/400\n",
      "15117/15117 [==============================] - 1s 89us/sample - loss: 30305021153.5008 - val_loss: 28206634148.3457\n",
      "Epoch 304/400\n",
      "15117/15117 [==============================] - 1s 86us/sample - loss: 30278121310.3764 - val_loss: 28195446258.0938\n",
      "Epoch 305/400\n",
      "15117/15117 [==============================] - 1s 63us/sample - loss: 30255877603.0419 - val_loss: 28188474572.8000\n",
      "Epoch 306/400\n",
      "15117/15117 [==============================] - 1s 57us/sample - loss: 30314766489.7660 - val_loss: 28232975944.0593\n",
      "Epoch 307/400\n",
      "15117/15117 [==============================] - 1s 58us/sample - loss: 30264461464.2757 - val_loss: 28179173229.3531\n",
      "Epoch 308/400\n",
      "15117/15117 [==============================] - 1s 58us/sample - loss: 30262361231.6730 - val_loss: 28190358257.4617\n",
      "Epoch 309/400\n",
      "15117/15117 [==============================] - 1s 57us/sample - loss: 30224842774.2182 - val_loss: 28134480413.0765\n",
      "Epoch 310/400\n",
      "15117/15117 [==============================] - 1s 58us/sample - loss: 30208915691.4584 - val_loss: 28180366818.9235\n",
      "Epoch 311/400\n",
      "15117/15117 [==============================] - 1s 57us/sample - loss: 30200775894.9337 - val_loss: 28119720340.5432\n",
      "Epoch 312/400\n",
      "15117/15117 [==============================] - 1s 57us/sample - loss: 30238042288.5260 - val_loss: 28164338998.9926\n",
      "Epoch 313/400\n",
      "15117/15117 [==============================] - 1s 57us/sample - loss: 30185564205.4863 - val_loss: 28132829563.2593\n",
      "Epoch 314/400\n",
      "15117/15117 [==============================] - 1s 58us/sample - loss: 30199835579.9907 - val_loss: 28162232481.8173\n",
      "Epoch 315/400\n",
      "15117/15117 [==============================] - 1s 57us/sample - loss: 30182767483.7071 - val_loss: 28072218497.5802\n",
      "Epoch 316/400\n",
      "15117/15117 [==============================] - 1s 58us/sample - loss: 30177327926.0044 - val_loss: 28130357384.5333\n",
      "Epoch 317/400\n",
      "15117/15117 [==============================] - 1s 59us/sample - loss: 30168493549.3381 - val_loss: 28045709726.6568\n",
      "Epoch 318/400\n",
      "15117/15117 [==============================] - 1s 57us/sample - loss: 30171526598.2531 - val_loss: 28022073015.3086\n",
      "Epoch 319/400\n",
      "15117/15117 [==============================] - 1s 58us/sample - loss: 30159677184.1693 - val_loss: 28077450722.9235\n",
      "Epoch 320/400\n",
      "15117/15117 [==============================] - 1s 58us/sample - loss: 30111164102.6426 - val_loss: 28036055929.9951\n",
      "Epoch 321/400\n",
      "15117/15117 [==============================] - 1s 58us/sample - loss: 30100240871.1400 - val_loss: 28000550497.3432\n",
      "Epoch 322/400\n",
      "15117/15117 [==============================] - 1s 58us/sample - loss: 30129633071.0951 - val_loss: 27997158614.9136\n",
      "Epoch 323/400\n",
      "15117/15117 [==============================] - 1s 58us/sample - loss: 30099867553.1664 - val_loss: 28032220769.3432\n",
      "Epoch 324/400\n",
      "15117/15117 [==============================] - 1s 57us/sample - loss: 30153111643.8531 - val_loss: 27992826134.1235\n",
      "Epoch 325/400\n",
      "15117/15117 [==============================] - 1s 59us/sample - loss: 30084997717.7228 - val_loss: 27979086274.0543\n",
      "Epoch 326/400\n",
      "15117/15117 [==============================] - 1s 60us/sample - loss: 30094607740.7570 - val_loss: 28049970800.5136\n",
      "Epoch 327/400\n",
      "15117/15117 [==============================] - 1s 58us/sample - loss: 30061112825.8019 - val_loss: 28005815040.6321\n",
      "Epoch 328/400\n",
      "15117/15117 [==============================] - 1s 57us/sample - loss: 30067618760.6578 - val_loss: 28064318876.1284\n",
      "Epoch 329/400\n",
      "15117/15117 [==============================] - 1s 59us/sample - loss: 30073462992.7017 - val_loss: 27957386667.2988\n",
      "Epoch 330/400\n",
      "15117/15117 [==============================] - 1s 57us/sample - loss: 30049199245.3699 - val_loss: 28121054276.2667\n",
      "Epoch 331/400\n",
      "15117/15117 [==============================] - 1s 57us/sample - loss: 30037976489.5998 - val_loss: 27919065947.6543\n",
      "Epoch 332/400\n",
      "15117/15117 [==============================] - 1s 58us/sample - loss: 30051884542.8146 - val_loss: 27931584542.3407\n",
      "Epoch 333/400\n",
      "15117/15117 [==============================] - 1s 58us/sample - loss: 30028053000.9076 - val_loss: 28017467199.8420\n",
      "Epoch 334/400\n",
      "15117/15117 [==============================] - 1s 59us/sample - loss: 30003577475.5139 - val_loss: 27876166587.7333\n",
      "Epoch 335/400\n",
      "15117/15117 [==============================] - 1s 58us/sample - loss: 29993462359.4840 - val_loss: 27868338100.1481\n",
      "Epoch 336/400\n",
      "15117/15117 [==============================] - 1s 55us/sample - loss: 30005334888.8758 - val_loss: 28058303065.7580\n",
      "Epoch 337/400\n",
      "15117/15117 [==============================] - 1s 54us/sample - loss: 29980698891.1599 - val_loss: 27877660409.0469\n",
      "Epoch 338/400\n",
      "15117/15117 [==============================] - 1s 52us/sample - loss: 29982257961.6083 - val_loss: 27878358286.5383\n",
      "Epoch 339/400\n",
      "15117/15117 [==============================] - 1s 54us/sample - loss: 29965113437.4789 - val_loss: 27855658335.4469\n",
      "Epoch 340/400\n",
      "15117/15117 [==============================] - 1s 52us/sample - loss: 29963524074.0528 - val_loss: 27836561499.0222\n",
      "Epoch 341/400\n",
      "15117/15117 [==============================] - 1s 52us/sample - loss: 29927553898.2983 - val_loss: 27784418857.7185\n",
      "Epoch 342/400\n",
      "15117/15117 [==============================] - 1s 53us/sample - loss: 29948512096.1376 - val_loss: 27807732278.3605\n",
      "Epoch 343/400\n",
      "15117/15117 [==============================] - 1s 52us/sample - loss: 29916335168.0127 - val_loss: 27801688600.0198\n",
      "Epoch 344/400\n",
      "15117/15117 [==============================] - 1s 52us/sample - loss: 29901445729.2383 - val_loss: 27790990439.6642\n",
      "Epoch 345/400\n",
      "15117/15117 [==============================] - 1s 52us/sample - loss: 29960203321.4421 - val_loss: 27776512768.6321\n",
      "Epoch 346/400\n",
      "15117/15117 [==============================] - 1s 52us/sample - loss: 29879916442.9344 - val_loss: 27808546664.2963\n",
      "Epoch 347/400\n",
      "15117/15117 [==============================] - 1s 52us/sample - loss: 29925340922.5979 - val_loss: 27782257787.8914\n",
      "Epoch 348/400\n",
      "15117/15117 [==============================] - 1s 53us/sample - loss: 29900779576.7647 - val_loss: 27754459752.9284\n",
      "Epoch 349/400\n",
      "15117/15117 [==============================] - 1s 52us/sample - loss: 29887351391.6126 - val_loss: 27724291479.0716\n",
      "Epoch 350/400\n",
      "15117/15117 [==============================] - 1s 52us/sample - loss: 29867984752.4287 - val_loss: 27688564953.4420\n",
      "Epoch 351/400\n",
      "15117/15117 [==============================] - 1s 52us/sample - loss: 29840415500.4808 - val_loss: 27689167065.4420\n",
      "Epoch 352/400\n",
      "15117/15117 [==============================] - 1s 53us/sample - loss: 29852418010.6085 - val_loss: 27781055950.6963\n",
      "Epoch 353/400\n",
      "15117/15117 [==============================] - 1s 52us/sample - loss: 29831734608.1852 - val_loss: 27673316354.5284\n",
      "Epoch 354/400\n",
      "15117/15117 [==============================] - 1s 53us/sample - loss: 29814779214.1531 - val_loss: 27681069111.6247\n",
      "Epoch 355/400\n",
      "15117/15117 [==============================] - 1s 55us/sample - loss: 29831745197.3085 - val_loss: 27630799009.8173\n",
      "Epoch 356/400\n",
      "15117/15117 [==============================] - 1s 54us/sample - loss: 29783944711.5528 - val_loss: 27623058687.3679\n",
      "Epoch 357/400\n",
      "15117/15117 [==============================] - 1s 53us/sample - loss: 29768808077.1328 - val_loss: 27700870773.5704\n",
      "Epoch 358/400\n",
      "15117/15117 [==============================] - 1s 53us/sample - loss: 29759501999.3406 - val_loss: 27598258555.2593\n",
      "Epoch 359/400\n",
      "15117/15117 [==============================] - 1s 52us/sample - loss: 29809640232.9309 - val_loss: 27571158301.7086\n",
      "Epoch 360/400\n",
      "15117/15117 [==============================] - 1s 52us/sample - loss: 29757772394.1798 - val_loss: 27569738425.8370\n",
      "Epoch 361/400\n",
      "15117/15117 [==============================] - 1s 52us/sample - loss: 29771021394.7085 - val_loss: 27651249058.4494\n",
      "Epoch 362/400\n",
      "15117/15117 [==============================] - 1s 52us/sample - loss: 29753764588.5083 - val_loss: 27579395013.8469\n",
      "Epoch 363/400\n",
      "15117/15117 [==============================] - 1s 52us/sample - loss: 29701941348.2527 - val_loss: 27554928933.2938\n",
      "Epoch 364/400\n",
      "15117/15117 [==============================] - 1s 52us/sample - loss: 29721888149.0073 - val_loss: 27562020156.0494\n",
      "Epoch 365/400\n",
      "15117/15117 [==============================] - 1s 53us/sample - loss: 29689801049.8718 - val_loss: 27530783938.6864\n",
      "Epoch 366/400\n",
      "15117/15117 [==============================] - 1s 52us/sample - loss: 29718943205.6159 - val_loss: 27525356450.4494\n",
      "Epoch 367/400\n",
      "15117/15117 [==============================] - 1s 52us/sample - loss: 29684495902.2452 - val_loss: 27540311527.9802\n",
      "Epoch 368/400\n",
      "15117/15117 [==============================] - 1s 53us/sample - loss: 29629179730.3190 - val_loss: 27646136390.7951\n",
      "Epoch 369/400\n",
      "15117/15117 [==============================] - 1s 52us/sample - loss: 29670649285.3048 - val_loss: 27498761258.9827\n",
      "Epoch 370/400\n",
      "15117/15117 [==============================] - 1s 52us/sample - loss: 29625029461.2317 - val_loss: 27489761340.6815\n",
      "Epoch 371/400\n",
      "15117/15117 [==============================] - 1s 53us/sample - loss: 29651788045.7339 - val_loss: 27481397460.3852\n",
      "Epoch 372/400\n",
      "15117/15117 [==============================] - 1s 53us/sample - loss: 29651617553.4595 - val_loss: 27437941089.9753\n",
      "Epoch 373/400\n",
      "15117/15117 [==============================] - 1s 52us/sample - loss: 29591788195.4864 - val_loss: 27477513769.7185\n",
      "Epoch 374/400\n",
      "15117/15117 [==============================] - 1s 53us/sample - loss: 29598005959.3877 - val_loss: 27432184703.0519\n",
      "Epoch 375/400\n",
      "15117/15117 [==============================] - 1s 52us/sample - loss: 29616889861.2836 - val_loss: 27449783996.3654\n",
      "Epoch 376/400\n",
      "15117/15117 [==============================] - 1s 53us/sample - loss: 29596731779.8696 - val_loss: 27365647964.2864\n",
      "Epoch 377/400\n",
      "15117/15117 [==============================] - 1s 52us/sample - loss: 29572207858.9096 - val_loss: 27374255111.5852\n",
      "Epoch 378/400\n",
      "15117/15117 [==============================] - 1s 53us/sample - loss: 29612634466.4068 - val_loss: 27387693748.7802\n",
      "Epoch 379/400\n",
      "15117/15117 [==============================] - 1s 53us/sample - loss: 29533767422.5944 - val_loss: 27365675589.5309\n",
      "Epoch 380/400\n",
      "15117/15117 [==============================] - 1s 52us/sample - loss: 29524879968.9674 - val_loss: 27498534846.2617\n",
      "Epoch 381/400\n",
      "15117/15117 [==============================] - 1s 52us/sample - loss: 29554267005.8747 - val_loss: 27334622336.9481\n",
      "Epoch 382/400\n",
      "15117/15117 [==============================] - 1s 52us/sample - loss: 29582214286.4537 - val_loss: 27380544337.5407\n",
      "Epoch 383/400\n",
      "15117/15117 [==============================] - 1s 53us/sample - loss: 29533784751.0019 - val_loss: 27345862094.6963\n",
      "Epoch 384/400\n",
      "15117/15117 [==============================] - 1s 52us/sample - loss: 29541379325.8154 - val_loss: 27350397560.0988\n",
      "Epoch 385/400\n",
      "15117/15117 [==============================] - 1s 51us/sample - loss: 29505568061.3540 - val_loss: 27266660867.7926\n",
      "Epoch 386/400\n",
      "15117/15117 [==============================] - 1s 52us/sample - loss: 29505342951.2416 - val_loss: 27530428289.5802\n",
      "Epoch 387/400\n",
      "15117/15117 [==============================] - 1s 52us/sample - loss: 29564697212.7740 - val_loss: 27536606301.5506\n",
      "Epoch 388/400\n",
      "15117/15117 [==============================] - 1s 52us/sample - loss: 29528478447.3533 - val_loss: 27250307168.0790\n",
      "Epoch 389/400\n",
      "15117/15117 [==============================] - 1s 52us/sample - loss: 29466506073.5670 - val_loss: 27238343030.2025\n",
      "Epoch 390/400\n",
      "15117/15117 [==============================] - 1s 52us/sample - loss: 29472499246.6378 - val_loss: 27276989553.7778\n",
      "Epoch 391/400\n",
      "15117/15117 [==============================] - 1s 52us/sample - loss: 29476797364.2008 - val_loss: 27198838617.1259\n",
      "Epoch 392/400\n",
      "15117/15117 [==============================] - 1s 52us/sample - loss: 29442567335.7201 - val_loss: 27181612484.5827\n",
      "Epoch 393/400\n",
      "15117/15117 [==============================] - 1s 52us/sample - loss: 29429746644.4443 - val_loss: 27178883491.7136\n",
      "Epoch 394/400\n",
      "15117/15117 [==============================] - 1s 53us/sample - loss: 29443669242.6995 - val_loss: 27305501309.1556\n",
      "Epoch 395/400\n",
      "15117/15117 [==============================] - 1s 52us/sample - loss: 29437734593.3590 - val_loss: 27166616851.5951\n",
      "Epoch 396/400\n",
      "15117/15117 [==============================] - 1s 52us/sample - loss: 29390105396.2432 - val_loss: 27202407477.0963\n",
      "Epoch 397/400\n",
      "15117/15117 [==============================] - 1s 52us/sample - loss: 29407342363.3155 - val_loss: 27163496190.1037\n",
      "Epoch 398/400\n",
      "15117/15117 [==============================] - 1s 52us/sample - loss: 29374375541.9324 - val_loss: 27128394648.3358\n",
      "Epoch 399/400\n",
      "15117/15117 [==============================] - 1s 52us/sample - loss: 29353416493.3339 - val_loss: 27167806810.3901\n",
      "Epoch 400/400\n",
      "15117/15117 [==============================] - 1s 53us/sample - loss: 29350026466.8556 - val_loss: 27100555476.3852\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1a65abf0388>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, \n",
    "          validation_data=(X_test, y_test),\n",
    "         batch_size=128, epochs=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explore model history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': [430240484408.9679,\n",
       "  429165608152.5594,\n",
       "  413150036138.1586,\n",
       "  332204597788.28076,\n",
       "  176851105679.14798,\n",
       "  102570620489.46219,\n",
       "  97810621257.58073,\n",
       "  96063904297.2865,\n",
       "  94298378049.24893,\n",
       "  92495514295.13051,\n",
       "  90635362011.77695,\n",
       "  88774720219.57373,\n",
       "  86742640205.25554,\n",
       "  84734287004.40775,\n",
       "  82555607587.32553,\n",
       "  80340144361.83264,\n",
       "  77984347098.26976,\n",
       "  75568967229.13382,\n",
       "  73112206236.6279,\n",
       "  70563563355.12495,\n",
       "  67992122103.48191,\n",
       "  65400138673.62678,\n",
       "  62896361896.04181,\n",
       "  60597845454.99133,\n",
       "  58488882372.37335,\n",
       "  56579355298.673546,\n",
       "  55019244173.26824,\n",
       "  53671466635.84573,\n",
       "  52502540561.83211,\n",
       "  51514944677.078255,\n",
       "  50620782640.90706,\n",
       "  49867204837.09096,\n",
       "  49105634873.06953,\n",
       "  48403078680.82609,\n",
       "  47785666970.62962,\n",
       "  47180328023.45016,\n",
       "  46620121530.39889,\n",
       "  46072989653.866776,\n",
       "  45604684182.7008,\n",
       "  45121519594.32374,\n",
       "  44692400160.1757,\n",
       "  44248758008.29477,\n",
       "  43873722281.02401,\n",
       "  43499424275.5425,\n",
       "  43108656256.63505,\n",
       "  42786334217.38175,\n",
       "  42453025865.63154,\n",
       "  42165797936.9748,\n",
       "  41846749484.351654,\n",
       "  41542698370.04062,\n",
       "  41245940287.23371,\n",
       "  40938106727.99524,\n",
       "  40637652978.858765,\n",
       "  40426452535.30833,\n",
       "  40143247042.40894,\n",
       "  39863939158.29861,\n",
       "  39596380140.49136,\n",
       "  39410742127.98836,\n",
       "  39168313171.33505,\n",
       "  38925720379.25408,\n",
       "  38704056830.74684,\n",
       "  38523148359.2607,\n",
       "  38327819671.03949,\n",
       "  38139129399.4438,\n",
       "  38034357396.55011,\n",
       "  37878574717.07879,\n",
       "  37681423759.04637,\n",
       "  37622607906.54654,\n",
       "  37406358846.70874,\n",
       "  37283647889.89138,\n",
       "  37145693383.895744,\n",
       "  37046710390.67751,\n",
       "  36937749661.35609,\n",
       "  36810433351.345375,\n",
       "  36685138230.64788,\n",
       "  36577746115.69597,\n",
       "  36498792435.60389,\n",
       "  36392523984.90494,\n",
       "  36322227603.5171,\n",
       "  36197816776.96262,\n",
       "  36077967158.74949,\n",
       "  36014578058.10148,\n",
       "  35928237329.45955,\n",
       "  35813229462.93788,\n",
       "  35732049846.165245,\n",
       "  35647604756.93114,\n",
       "  35547475219.01753,\n",
       "  35475953377.46696,\n",
       "  35375326988.006615,\n",
       "  35299946993.334656,\n",
       "  35222755697.98664,\n",
       "  35129872277.650856,\n",
       "  35027741032.63875,\n",
       "  34961792436.09923,\n",
       "  34843970758.13455,\n",
       "  34766236395.153534,\n",
       "  34720671115.52398,\n",
       "  34608881684.21989,\n",
       "  34562091962.29728,\n",
       "  34482313411.2218,\n",
       "  34436352152.682144,\n",
       "  34362006353.84481,\n",
       "  34335151027.658928,\n",
       "  34282057069.651386,\n",
       "  34253647268.925846,\n",
       "  34186974017.45214,\n",
       "  34088370551.676655,\n",
       "  34061771012.65701,\n",
       "  34032715057.973938,\n",
       "  33964306250.054905,\n",
       "  33880070495.968246,\n",
       "  33837094188.55487,\n",
       "  33807498312.141296,\n",
       "  33738423602.414234,\n",
       "  33712884874.05067,\n",
       "  33707381997.01634,\n",
       "  33625151392.082558,\n",
       "  33559242105.03142,\n",
       "  33576076212.065357,\n",
       "  33527861952.952568,\n",
       "  33467886165.993782,\n",
       "  33436433222.871204,\n",
       "  33391209953.822582,\n",
       "  33349879967.185024,\n",
       "  33303194844.62367,\n",
       "  33267473312.82768,\n",
       "  33231813908.710987,\n",
       "  33202985783.968777,\n",
       "  33189955412.960773,\n",
       "  33165480641.968643,\n",
       "  33077644703.98095,\n",
       "  33148038742.603428,\n",
       "  33036316908.338955,\n",
       "  33026556251.836212,\n",
       "  32958216286.257854,\n",
       "  32941094309.94192,\n",
       "  32894238385.64371,\n",
       "  32865756094.835747,\n",
       "  32842729397.081432,\n",
       "  32850022178.902164,\n",
       "  32776317533.61434,\n",
       "  32757909982.4018,\n",
       "  32726618593.21294,\n",
       "  32712390228.232586,\n",
       "  32682656284.41622,\n",
       "  32668652412.248993,\n",
       "  32601385242.468742,\n",
       "  32580040551.317856,\n",
       "  32529006232.17411,\n",
       "  32506496509.86624,\n",
       "  32493689104.714428,\n",
       "  32497481376.709137,\n",
       "  32429847629.424885,\n",
       "  32438861902.44096,\n",
       "  32378289028.919495,\n",
       "  32382904523.621353,\n",
       "  32365264032.844612,\n",
       "  32289116222.996628,\n",
       "  32305353989.131176,\n",
       "  32283090731.03076,\n",
       "  32292996753.738968,\n",
       "  32214672002.56559,\n",
       "  32234053328.26143,\n",
       "  32224261935.907917,\n",
       "  32171273230.02183,\n",
       "  32127231834.108883,\n",
       "  32127810367.758682,\n",
       "  32064889886.00807,\n",
       "  32074777722.538597,\n",
       "  32037166879.108818,\n",
       "  32014071884.612026,\n",
       "  31982921634.995304,\n",
       "  31991833026.188793,\n",
       "  31993878556.721043,\n",
       "  31920871901.55507,\n",
       "  31913486825.20606,\n",
       "  31959145016.3244,\n",
       "  31882703221.91546,\n",
       "  31895020975.83171,\n",
       "  31829744498.901104,\n",
       "  31846995298.880997,\n",
       "  31794734552.068268,\n",
       "  31804260276.81048,\n",
       "  31774053772.777138,\n",
       "  31718084380.873455,\n",
       "  31735906229.013695,\n",
       "  31740192662.395977,\n",
       "  31679896596.931137,\n",
       "  31698370050.066017,\n",
       "  31668163809.36535,\n",
       "  31732024788.4104,\n",
       "  31608331547.75577,\n",
       "  31582493943.244823,\n",
       "  31593611136.922935,\n",
       "  31542645469.605873,\n",
       "  31559770440.124363,\n",
       "  31559956673.32513,\n",
       "  31523879419.901833,\n",
       "  31486181129.703514,\n",
       "  31490551336.4059,\n",
       "  31470533215.61262,\n",
       "  31439203979.778,\n",
       "  31448688224.76417,\n",
       "  31369742062.743668,\n",
       "  31407505542.663757,\n",
       "  31384170437.54184,\n",
       "  31329479825.434147,\n",
       "  31392203475.44513,\n",
       "  31312879480.15082,\n",
       "  31298640206.390156,\n",
       "  31301793647.852882,\n",
       "  31308915592.441887,\n",
       "  31277916859.872196,\n",
       "  31236503502.415558,\n",
       "  31225325353.269566,\n",
       "  31224924739.433487,\n",
       "  31195173350.564266,\n",
       "  31182948578.923332,\n",
       "  31155783507.53827,\n",
       "  31183337811.572136,\n",
       "  31176509844.99041,\n",
       "  31141869759.42846,\n",
       "  31120677305.99246,\n",
       "  31156785124.633724,\n",
       "  31099625040.100548,\n",
       "  31107851763.29907,\n",
       "  31063221526.743137,\n",
       "  31070662142.543625,\n",
       "  31044915715.89495,\n",
       "  31023962167.545414,\n",
       "  31030651110.5812,\n",
       "  31007015370.114178,\n",
       "  30982210585.266388,\n",
       "  30981331239.745453,\n",
       "  30989666516.56281,\n",
       "  30960917988.05795,\n",
       "  30937836222.44625,\n",
       "  30938349542.80135,\n",
       "  30920809671.218365,\n",
       "  30929301439.580868,\n",
       "  30935063909.45505,\n",
       "  30921444265.024014,\n",
       "  30912539336.336044,\n",
       "  30875481613.852486,\n",
       "  30863913502.583847,\n",
       "  30865275076.271748,\n",
       "  30886288620.068005,\n",
       "  30872681089.752728,\n",
       "  30818625490.34438,\n",
       "  30761856156.54323,\n",
       "  30798927405.689487,\n",
       "  30782206074.741814,\n",
       "  30802638749.23755,\n",
       "  30758864800.759937,\n",
       "  30780973516.620495,\n",
       "  30742535568.672092,\n",
       "  30722842012.187603,\n",
       "  30725581143.162266,\n",
       "  30711150089.280148,\n",
       "  30699683915.59595,\n",
       "  30699879063.970894,\n",
       "  30685175822.563736,\n",
       "  30694538412.12304,\n",
       "  30642348459.767414,\n",
       "  30647009283.386917,\n",
       "  30659470670.288548,\n",
       "  30633604555.19799,\n",
       "  30603104579.518158,\n",
       "  30603832052.636898,\n",
       "  30630889800.530792,\n",
       "  30627584679.753918,\n",
       "  30569901245.565655,\n",
       "  30561829475.33823,\n",
       "  30616906209.21294,\n",
       "  30577910922.79579,\n",
       "  30552085642.93127,\n",
       "  30566407730.90534,\n",
       "  30509338992.902824,\n",
       "  30502351791.459152,\n",
       "  30523374044.505127,\n",
       "  30515151314.445988,\n",
       "  30478739268.771317,\n",
       "  30454543283.52345,\n",
       "  30478548733.171925,\n",
       "  30447732599.304096,\n",
       "  30435611992.855724,\n",
       "  30442312362.395714,\n",
       "  30442549701.98214,\n",
       "  30450024120.688496,\n",
       "  30402870167.47979,\n",
       "  30386350791.828007,\n",
       "  30435643059.811337,\n",
       "  30390455471.103527,\n",
       "  30405520290.72435,\n",
       "  30374063297.528347,\n",
       "  30361463369.22511,\n",
       "  30393201961.574387,\n",
       "  30351688522.29199,\n",
       "  30364084655.899452,\n",
       "  30347432604.17067,\n",
       "  30289034458.18509,\n",
       "  30272845201.755905,\n",
       "  30305021153.500828,\n",
       "  30278121310.376396,\n",
       "  30255877603.041874,\n",
       "  30314766489.76596,\n",
       "  30264461464.275715,\n",
       "  30262361231.67295,\n",
       "  30224842774.218166,\n",
       "  30208915691.45836,\n",
       "  30200775894.93365,\n",
       "  30238042288.52603,\n",
       "  30185564205.486275,\n",
       "  30199835579.990738,\n",
       "  30182767483.707085,\n",
       "  30177327926.004368,\n",
       "  30168493549.338097,\n",
       "  30171526598.253094,\n",
       "  30159677184.169346,\n",
       "  30111164102.64259,\n",
       "  30100240871.14004,\n",
       "  30129633071.09506,\n",
       "  30099867553.16637,\n",
       "  30153111643.853146,\n",
       "  30084997717.722828,\n",
       "  30094607740.757027,\n",
       "  30061112825.801945,\n",
       "  30067618760.657803,\n",
       "  30073462992.701725,\n",
       "  30049199245.36985,\n",
       "  30037976489.59979,\n",
       "  30051884542.81458,\n",
       "  30028053000.90759,\n",
       "  30003577475.513924,\n",
       "  29993462359.484024,\n",
       "  30005334888.875835,\n",
       "  29980698891.159885,\n",
       "  29982257961.608257,\n",
       "  29965113437.478867,\n",
       "  29963524074.052788,\n",
       "  29927553898.29834,\n",
       "  29948512096.137592,\n",
       "  29916335168.0127,\n",
       "  29901445729.238342,\n",
       "  29960203321.442085,\n",
       "  29879916442.934444,\n",
       "  29925340922.59787,\n",
       "  29900779576.7647,\n",
       "  29887351391.61262,\n",
       "  29867984752.428658,\n",
       "  29840415500.48078,\n",
       "  29852418010.608456,\n",
       "  29831734608.185223,\n",
       "  29814779214.153072,\n",
       "  29831745197.30846,\n",
       "  29783944711.552822,\n",
       "  29768808077.132763,\n",
       "  29759501999.34061,\n",
       "  29809640232.930874,\n",
       "  29757772394.1798,\n",
       "  29771021394.708473,\n",
       "  29753764588.5083,\n",
       "  29701941348.252697,\n",
       "  29721888149.007343,\n",
       "  29689801049.8718,\n",
       "  29718943205.61593,\n",
       "  29684495902.245155,\n",
       "  29629179730.318977,\n",
       "  29670649285.304756,\n",
       "  29625029461.231728,\n",
       "  29651788045.73394,\n",
       "  29651617553.45955,\n",
       "  29591788195.486404,\n",
       "  29598005959.38771,\n",
       "  29616889861.28359,\n",
       "  29596731779.869553,\n",
       "  29572207858.909573,\n",
       "  29612634466.406826,\n",
       "  29533767422.59443,\n",
       "  29524879968.96739,\n",
       "  29554267005.87471,\n",
       "  29582214286.453663,\n",
       "  29533784751.00192,\n",
       "  29541379325.81544,\n",
       "  29505568061.353973,\n",
       "  29505342951.24165,\n",
       "  29564697212.773964,\n",
       "  29528478447.35331,\n",
       "  29466506073.56698,\n",
       "  29472499246.637825,\n",
       "  29476797364.200832,\n",
       "  29442567335.72005,\n",
       "  29429746644.444267,\n",
       "  29443669242.69948,\n",
       "  29437734593.359,\n",
       "  29390105396.24317,\n",
       "  29407342363.31547,\n",
       "  29374375541.932392,\n",
       "  29353416493.333862,\n",
       "  29350026466.855595],\n",
       " 'val_loss': [418918654295.86176,\n",
       "  415111405345.5012,\n",
       "  377801060766.6568,\n",
       "  247490837759.3679,\n",
       "  110653639186.96297,\n",
       "  95476297434.70618,\n",
       "  93857973751.15062,\n",
       "  92241728226.29135,\n",
       "  90529396093.78766,\n",
       "  88801046730.2716,\n",
       "  86941292407.46666,\n",
       "  85091557376.0,\n",
       "  83440093007.01234,\n",
       "  81155310791.74321,\n",
       "  79142788240.11852,\n",
       "  76937602530.92346,\n",
       "  74550713275.73334,\n",
       "  72378061993.40247,\n",
       "  69770928724.70123,\n",
       "  67308453867.77284,\n",
       "  64783410770.17284,\n",
       "  62441260696.9679,\n",
       "  60123488498.72593,\n",
       "  57908003938.60741,\n",
       "  55979829920.553085,\n",
       "  54350924701.39259,\n",
       "  52870521671.42716,\n",
       "  51661773866.98272,\n",
       "  50627627943.50617,\n",
       "  49777420207.091354,\n",
       "  49034525293.98518,\n",
       "  48169279103.68395,\n",
       "  47487804742.162964,\n",
       "  46915701352.9284,\n",
       "  46264909975.703705,\n",
       "  45649781317.53086,\n",
       "  45098436208.51358,\n",
       "  44609805592.651855,\n",
       "  44126976192.15803,\n",
       "  43713637590.91358,\n",
       "  43241290087.0321,\n",
       "  42867667550.81481,\n",
       "  42485837743.091354,\n",
       "  42118180621.27407,\n",
       "  41914019774.26173,\n",
       "  41458066788.5037,\n",
       "  41182252418.844444,\n",
       "  40938137888.23704,\n",
       "  40527067065.20494,\n",
       "  40241775249.38271,\n",
       "  39922780873.00741,\n",
       "  39648553045.96543,\n",
       "  39366090620.52346,\n",
       "  39066699558.55802,\n",
       "  38807615037.94568,\n",
       "  38579929431.861725,\n",
       "  38317568591.64445,\n",
       "  38118246546.64691,\n",
       "  37818196362.42963,\n",
       "  37600851007.20988,\n",
       "  37383157942.04444,\n",
       "  37184661266.330864,\n",
       "  37014810037.412346,\n",
       "  36921983461.45185,\n",
       "  36674676437.64938,\n",
       "  36524587994.07407,\n",
       "  36422696034.60741,\n",
       "  36250151068.76049,\n",
       "  36112626291.04198,\n",
       "  36041197896.69136,\n",
       "  35883583078.4,\n",
       "  35752288711.111115,\n",
       "  35720515199.68395,\n",
       "  35609582028.1679,\n",
       "  35422817899.45679,\n",
       "  35376650846.81481,\n",
       "  35232623264.553085,\n",
       "  35113992283.022224,\n",
       "  35130345032.05926,\n",
       "  35103628260.18765,\n",
       "  34826873019.101234,\n",
       "  34739730796.08889,\n",
       "  34655107785.00741,\n",
       "  34607981191.269135,\n",
       "  34497828515.08148,\n",
       "  34405889183.28889,\n",
       "  34284806073.204937,\n",
       "  34212541103.723457,\n",
       "  34145291893.57037,\n",
       "  34058598582.044445,\n",
       "  33948123318.044445,\n",
       "  33896557823.3679,\n",
       "  33766129613.4321,\n",
       "  33675609500.128395,\n",
       "  33612666379.377777,\n",
       "  33509224604.760494,\n",
       "  33464155128.414814,\n",
       "  33418542876.444443,\n",
       "  33298706300.523457,\n",
       "  33234952788.701233,\n",
       "  33265777798.00494,\n",
       "  33123159442.014816,\n",
       "  33056992668.128395,\n",
       "  32996472381.94568,\n",
       "  33006374669.274075,\n",
       "  32884429588.85926,\n",
       "  32843387593.00741,\n",
       "  32775962947.634567,\n",
       "  32700359303.269135,\n",
       "  32653337137.303703,\n",
       "  32631442813.787655,\n",
       "  32538620308.54321,\n",
       "  32488435792.90864,\n",
       "  32448276035.00247,\n",
       "  32446129452.879013,\n",
       "  32313967886.538273,\n",
       "  32282025028.266666,\n",
       "  32374431076.503704,\n",
       "  32219383206.241974,\n",
       "  32184190733.274075,\n",
       "  32183101369.204937,\n",
       "  32027666715.18025,\n",
       "  31993957686.99259,\n",
       "  31947742776.88889,\n",
       "  31910410174.26173,\n",
       "  31856601993.165432,\n",
       "  31941756022.834568,\n",
       "  31767906192.750618,\n",
       "  31730184940.404938,\n",
       "  31693651876.97778,\n",
       "  31669236490.745678,\n",
       "  31623460009.40247,\n",
       "  31594437300.780247,\n",
       "  31522247980.879013,\n",
       "  31488069980.918518,\n",
       "  31446767701.96543,\n",
       "  31407446425.6,\n",
       "  31376897815.387653,\n",
       "  31397194471.34815,\n",
       "  31302879075.239506,\n",
       "  31265782806.755554,\n",
       "  31261429537.501236,\n",
       "  31223891876.97778,\n",
       "  31145242138.54815,\n",
       "  31189318112.39506,\n",
       "  31107449150.577778,\n",
       "  31022722126.380245,\n",
       "  30997203035.02222,\n",
       "  30983269924.661728,\n",
       "  31005215463.34815,\n",
       "  30914367144.13827,\n",
       "  30948705479.74321,\n",
       "  30853032532.701233,\n",
       "  30908820174.064198,\n",
       "  30913127527.664196,\n",
       "  30759604835.871605,\n",
       "  30795593439.762962,\n",
       "  30823494845.62963,\n",
       "  30700769446.874073,\n",
       "  30632860644.187653,\n",
       "  30644384305.303703,\n",
       "  30598623459.555557,\n",
       "  30609338691.634567,\n",
       "  30542830771.51605,\n",
       "  30524813438.419754,\n",
       "  30514478153.323456,\n",
       "  30484384558.14321,\n",
       "  30502820279.940742,\n",
       "  30396163726.85432,\n",
       "  30413839585.02716,\n",
       "  30364695837.70864,\n",
       "  30313014340.266666,\n",
       "  30288653931.45679,\n",
       "  30303969219.31852,\n",
       "  30244748209.619755,\n",
       "  30215733961.00741,\n",
       "  30179313042.014816,\n",
       "  30217693037.353085,\n",
       "  30146608975.012344,\n",
       "  30104670390.044445,\n",
       "  30111977340.523457,\n",
       "  30038987368.928394,\n",
       "  30062376034.607407,\n",
       "  29989583085.669136,\n",
       "  30086651590.47901,\n",
       "  30034521495.071606,\n",
       "  29938559926.676544,\n",
       "  29922772066.607407,\n",
       "  29930592918.439507,\n",
       "  30189019247.249382,\n",
       "  29906843076.582718,\n",
       "  29854677967.960495,\n",
       "  29846941321.79753,\n",
       "  29773800981.49136,\n",
       "  29763029320.691357,\n",
       "  29765748559.012344,\n",
       "  29849708442.864197,\n",
       "  29820261631.3679,\n",
       "  29696175192.493828,\n",
       "  29697357424.51358,\n",
       "  29679574964.148148,\n",
       "  29609415849.40247,\n",
       "  29682896734.182716,\n",
       "  29712948272.039505,\n",
       "  29649595971.00247,\n",
       "  29558204815.48642,\n",
       "  29986111189.649384,\n",
       "  29554836664.572838,\n",
       "  29501352649.00741,\n",
       "  29712056679.032097,\n",
       "  29456371656.37531,\n",
       "  29433697762.92346,\n",
       "  29456543359.683952,\n",
       "  29407222887.664196,\n",
       "  29403659524.42469,\n",
       "  29381171968.6321,\n",
       "  29362541970.014816,\n",
       "  29322089742.538273,\n",
       "  29295864318.7358,\n",
       "  29284867605.49136,\n",
       "  29290117661.07654,\n",
       "  29259474999.62469,\n",
       "  29239062280.217285,\n",
       "  29250844813.590122,\n",
       "  29209844784.039505,\n",
       "  29204062344.533333,\n",
       "  29272955800.335804,\n",
       "  29177036648.296295,\n",
       "  29241395473.066666,\n",
       "  29167960544.39506,\n",
       "  29109182362.864197,\n",
       "  29089605535.920986,\n",
       "  29104150947.71358,\n",
       "  29125204342.20247,\n",
       "  29060972872.691357,\n",
       "  29068116969.244446,\n",
       "  29009857184.553085,\n",
       "  29035827933.23457,\n",
       "  28981946110.103703,\n",
       "  29095101093.609875,\n",
       "  29000716824.019753,\n",
       "  28951756987.101234,\n",
       "  28946198608.90864,\n",
       "  28923598499.081482,\n",
       "  28945060654.14321,\n",
       "  29027462510.617283,\n",
       "  28873253420.246914,\n",
       "  28973452538.31111,\n",
       "  28882787436.72099,\n",
       "  28910212533.412346,\n",
       "  28830667202.05432,\n",
       "  28845759546.153088,\n",
       "  28821546310.162964,\n",
       "  28780435825.14568,\n",
       "  28757509724.28642,\n",
       "  28771903680.158024,\n",
       "  28733565236.4642,\n",
       "  28735889165.274075,\n",
       "  28689818019.71358,\n",
       "  28712304589.4321,\n",
       "  28672763317.412346,\n",
       "  28787349549.511112,\n",
       "  28677566974.7358,\n",
       "  28746013157.45185,\n",
       "  28695520607.446915,\n",
       "  28658109922.92346,\n",
       "  28670860836.661728,\n",
       "  28620838669.274075,\n",
       "  28632227445.57037,\n",
       "  28646396450.133335,\n",
       "  28608778882.212345,\n",
       "  28627494360.809875,\n",
       "  28594319461.135803,\n",
       "  28563238482.17284,\n",
       "  28529592972.325928,\n",
       "  28552084335.88148,\n",
       "  28578469089.02716,\n",
       "  28517923250.88395,\n",
       "  28474113259.14074,\n",
       "  28516183315.595062,\n",
       "  28597884240.276543,\n",
       "  28442471659.14074,\n",
       "  28474870892.72099,\n",
       "  28457299057.77778,\n",
       "  28431011334.320988,\n",
       "  28410227067.25926,\n",
       "  28436008995.39753,\n",
       "  28377798731.851852,\n",
       "  28378237732.02963,\n",
       "  28337936368.829628,\n",
       "  28330359486.893826,\n",
       "  28363116564.22716,\n",
       "  28366710849.73827,\n",
       "  28370379333.530865,\n",
       "  28301631311.012344,\n",
       "  28285805342.97284,\n",
       "  28283943852.56296,\n",
       "  28261430274.528397,\n",
       "  28269048515.95062,\n",
       "  28261667382.360493,\n",
       "  28271571060.30617,\n",
       "  28208943189.96543,\n",
       "  28206634148.34568,\n",
       "  28195446258.093826,\n",
       "  28188474572.8,\n",
       "  28232975944.059258,\n",
       "  28179173229.353085,\n",
       "  28190358257.461727,\n",
       "  28134480413.07654,\n",
       "  28180366818.92346,\n",
       "  28119720340.54321,\n",
       "  28164338998.99259,\n",
       "  28132829563.25926,\n",
       "  28162232481.817284,\n",
       "  28072218497.580246,\n",
       "  28130357384.533333,\n",
       "  28045709726.65679,\n",
       "  28022073015.308643,\n",
       "  28077450722.92346,\n",
       "  28036055929.99506,\n",
       "  28000550497.34321,\n",
       "  27997158614.91358,\n",
       "  28032220769.34321,\n",
       "  27992826134.123455,\n",
       "  27979086274.05432,\n",
       "  28049970800.51358,\n",
       "  28005815040.6321,\n",
       "  28064318876.128395,\n",
       "  27957386667.298767,\n",
       "  28121054276.266666,\n",
       "  27919065947.65432,\n",
       "  27931584542.34074,\n",
       "  28017467199.841976,\n",
       "  27876166587.733334,\n",
       "  27868338100.148148,\n",
       "  28058303065.758026,\n",
       "  27877660409.046913,\n",
       "  27878358286.538273,\n",
       "  27855658335.446915,\n",
       "  27836561499.02222,\n",
       "  27784418857.718517,\n",
       "  27807732278.360493,\n",
       "  27801688600.019753,\n",
       "  27790990439.664196,\n",
       "  27776512768.6321,\n",
       "  27808546664.296295,\n",
       "  27782257787.891357,\n",
       "  27754459752.928394,\n",
       "  27724291479.071606,\n",
       "  27688564953.441975,\n",
       "  27689167065.441975,\n",
       "  27781055950.696297,\n",
       "  27673316354.528397,\n",
       "  27681069111.62469,\n",
       "  27630799009.817284,\n",
       "  27623058687.3679,\n",
       "  27700870773.57037,\n",
       "  27598258555.25926,\n",
       "  27571158301.70864,\n",
       "  27569738425.837036,\n",
       "  27651249058.449383,\n",
       "  27579395013.846912,\n",
       "  27554928933.293827,\n",
       "  27562020156.04938,\n",
       "  27530783938.68642,\n",
       "  27525356450.449383,\n",
       "  27540311527.980247,\n",
       "  27646136390.795063,\n",
       "  27498761258.982716,\n",
       "  27489761340.68148,\n",
       "  27481397460.385185,\n",
       "  27437941089.975307,\n",
       "  27477513769.718517,\n",
       "  27432184703.051853,\n",
       "  27449783996.365433,\n",
       "  27365647964.28642,\n",
       "  27374255111.585186,\n",
       "  27387693748.780247,\n",
       "  27365675589.530865,\n",
       "  27498534846.26173,\n",
       "  27334622336.948147,\n",
       "  27380544337.54074,\n",
       "  27345862094.696297,\n",
       "  27350397560.098766,\n",
       "  27266660867.79259,\n",
       "  27530428289.580246,\n",
       "  27536606301.550617,\n",
       "  27250307168.079014,\n",
       "  27238343030.20247,\n",
       "  27276989553.77778,\n",
       "  27198838617.125927,\n",
       "  27181612484.582718,\n",
       "  27178883491.71358,\n",
       "  27305501309.155556,\n",
       "  27166616851.595062,\n",
       "  27202407477.096294,\n",
       "  27163496190.103703,\n",
       "  27128394648.335804,\n",
       "  27167806810.390125,\n",
       "  27100555476.385185]}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "      <th>val_loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.302405e+11</td>\n",
       "      <td>4.189187e+11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.291656e+11</td>\n",
       "      <td>4.151114e+11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.131500e+11</td>\n",
       "      <td>3.778011e+11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.322046e+11</td>\n",
       "      <td>2.474908e+11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.768511e+11</td>\n",
       "      <td>1.106536e+11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>395</th>\n",
       "      <td>2.939011e+10</td>\n",
       "      <td>2.720241e+10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>396</th>\n",
       "      <td>2.940734e+10</td>\n",
       "      <td>2.716350e+10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>397</th>\n",
       "      <td>2.937438e+10</td>\n",
       "      <td>2.712839e+10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>398</th>\n",
       "      <td>2.935342e+10</td>\n",
       "      <td>2.716781e+10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399</th>\n",
       "      <td>2.935003e+10</td>\n",
       "      <td>2.710056e+10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>400 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             loss      val_loss\n",
       "0    4.302405e+11  4.189187e+11\n",
       "1    4.291656e+11  4.151114e+11\n",
       "2    4.131500e+11  3.778011e+11\n",
       "3    3.322046e+11  2.474908e+11\n",
       "4    1.768511e+11  1.106536e+11\n",
       "..            ...           ...\n",
       "395  2.939011e+10  2.720241e+10\n",
       "396  2.940734e+10  2.716350e+10\n",
       "397  2.937438e+10  2.712839e+10\n",
       "398  2.935342e+10  2.716781e+10\n",
       "399  2.935003e+10  2.710056e+10\n",
       "\n",
       "[400 rows x 2 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(model.history.history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that when convert into a dataframe it show *loss* and *val_loss* columns;\n",
    "\n",
    "the <b>val_loss</b> is the loss on the test set (validation data).\n",
    "\n",
    "Now is possible directly compare the loss on training vs the loss on test (or validation) in order to see precence of <b>overfitting</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = pd.DataFrame(model.history.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1a65c4a8848>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "losses.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the behavior that we wanted to see, decrease in both variables, and no increase in validation set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Technically we can continue training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### If val_loss eventually increases, means overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation on test data\n",
    "\n",
    "Errors in terms of the target units (price, $USD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, explained_variance_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[406776.66],\n",
       "       [586576.5 ],\n",
       "       [591038.2 ],\n",
       "       ...,\n",
       "       [411804.16],\n",
       "       [565179.3 ],\n",
       "       [682256.2 ]], dtype=float32)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27100555269.39583"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_squared_error(y_test, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "164622.46283358728"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_squared_error(y_test, predictions) ** 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "101046.57492766203"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_absolute_error(y_test, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    2.159700e+04\n",
       "mean     5.402966e+05\n",
       "std      3.673681e+05\n",
       "min      7.800000e+04\n",
       "25%      3.220000e+05\n",
       "50%      4.500000e+05\n",
       "75%      6.450000e+05\n",
       "max      7.700000e+06\n",
       "Name: price, dtype: float64"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['price'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Error about 20%, not so good actually (but not horrible)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18.702056412655946"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_absolute_error(y_test, predictions) * 100 / 5.402966e+05  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why tho? we can use explained variance score (`best = 1.0`) to figure it out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7956523556462562"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "explained_variance_score(y_test, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depends on the context; \n",
    " * do we have previous model with better performance?\n",
    " * we could still training to keep lowering the loss since we are not overfitting yet.\n",
    " \n",
    "lets compare our predictions and plot them out vs a perfect fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1a65f4ea808>]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(12,6))\n",
    "plt.scatter(y_test, predictions)\n",
    "plt.plot(y_test, y_test, c='r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Currently we are not good predicting very expensive houses , but pretty good on houses from 0 to 2 million dollars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Maybe worth to re-train the model just on buttom 99% of houses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict the price of a brand new house!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>price</th>\n",
       "      <th>bedrooms</th>\n",
       "      <th>bathrooms</th>\n",
       "      <th>sqft_living</th>\n",
       "      <th>sqft_lot</th>\n",
       "      <th>floors</th>\n",
       "      <th>waterfront</th>\n",
       "      <th>view</th>\n",
       "      <th>condition</th>\n",
       "      <th>grade</th>\n",
       "      <th>sqft_above</th>\n",
       "      <th>sqft_basement</th>\n",
       "      <th>yr_built</th>\n",
       "      <th>yr_renovated</th>\n",
       "      <th>lat</th>\n",
       "      <th>long</th>\n",
       "      <th>sqft_living15</th>\n",
       "      <th>sqft_lot15</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>221900.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1180</td>\n",
       "      <td>5650</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>1180</td>\n",
       "      <td>0</td>\n",
       "      <td>1955</td>\n",
       "      <td>0</td>\n",
       "      <td>47.5112</td>\n",
       "      <td>-122.257</td>\n",
       "      <td>1340</td>\n",
       "      <td>5650</td>\n",
       "      <td>2014</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>538000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.25</td>\n",
       "      <td>2570</td>\n",
       "      <td>7242</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>2170</td>\n",
       "      <td>400</td>\n",
       "      <td>1951</td>\n",
       "      <td>1991</td>\n",
       "      <td>47.7210</td>\n",
       "      <td>-122.319</td>\n",
       "      <td>1690</td>\n",
       "      <td>7639</td>\n",
       "      <td>2014</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>180000.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1.00</td>\n",
       "      <td>770</td>\n",
       "      <td>10000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>770</td>\n",
       "      <td>0</td>\n",
       "      <td>1933</td>\n",
       "      <td>0</td>\n",
       "      <td>47.7379</td>\n",
       "      <td>-122.233</td>\n",
       "      <td>2720</td>\n",
       "      <td>8062</td>\n",
       "      <td>2015</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>604000.0</td>\n",
       "      <td>4</td>\n",
       "      <td>3.00</td>\n",
       "      <td>1960</td>\n",
       "      <td>5000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>1050</td>\n",
       "      <td>910</td>\n",
       "      <td>1965</td>\n",
       "      <td>0</td>\n",
       "      <td>47.5208</td>\n",
       "      <td>-122.393</td>\n",
       "      <td>1360</td>\n",
       "      <td>5000</td>\n",
       "      <td>2014</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>510000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.00</td>\n",
       "      <td>1680</td>\n",
       "      <td>8080</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>1680</td>\n",
       "      <td>0</td>\n",
       "      <td>1987</td>\n",
       "      <td>0</td>\n",
       "      <td>47.6168</td>\n",
       "      <td>-122.045</td>\n",
       "      <td>1800</td>\n",
       "      <td>7503</td>\n",
       "      <td>2015</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21592</th>\n",
       "      <td>360000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.50</td>\n",
       "      <td>1530</td>\n",
       "      <td>1131</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>1530</td>\n",
       "      <td>0</td>\n",
       "      <td>2009</td>\n",
       "      <td>0</td>\n",
       "      <td>47.6993</td>\n",
       "      <td>-122.346</td>\n",
       "      <td>1530</td>\n",
       "      <td>1509</td>\n",
       "      <td>2014</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21593</th>\n",
       "      <td>400000.0</td>\n",
       "      <td>4</td>\n",
       "      <td>2.50</td>\n",
       "      <td>2310</td>\n",
       "      <td>5813</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>2310</td>\n",
       "      <td>0</td>\n",
       "      <td>2014</td>\n",
       "      <td>0</td>\n",
       "      <td>47.5107</td>\n",
       "      <td>-122.362</td>\n",
       "      <td>1830</td>\n",
       "      <td>7200</td>\n",
       "      <td>2015</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21594</th>\n",
       "      <td>402101.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.75</td>\n",
       "      <td>1020</td>\n",
       "      <td>1350</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>1020</td>\n",
       "      <td>0</td>\n",
       "      <td>2009</td>\n",
       "      <td>0</td>\n",
       "      <td>47.5944</td>\n",
       "      <td>-122.299</td>\n",
       "      <td>1020</td>\n",
       "      <td>2007</td>\n",
       "      <td>2014</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21595</th>\n",
       "      <td>400000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.50</td>\n",
       "      <td>1600</td>\n",
       "      <td>2388</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>1600</td>\n",
       "      <td>0</td>\n",
       "      <td>2004</td>\n",
       "      <td>0</td>\n",
       "      <td>47.5345</td>\n",
       "      <td>-122.069</td>\n",
       "      <td>1410</td>\n",
       "      <td>1287</td>\n",
       "      <td>2015</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21596</th>\n",
       "      <td>325000.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.75</td>\n",
       "      <td>1020</td>\n",
       "      <td>1076</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>1020</td>\n",
       "      <td>0</td>\n",
       "      <td>2008</td>\n",
       "      <td>0</td>\n",
       "      <td>47.5941</td>\n",
       "      <td>-122.299</td>\n",
       "      <td>1020</td>\n",
       "      <td>1357</td>\n",
       "      <td>2014</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>21597 rows × 20 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          price  bedrooms  bathrooms  sqft_living  sqft_lot  floors  \\\n",
       "0      221900.0         3       1.00         1180      5650     1.0   \n",
       "1      538000.0         3       2.25         2570      7242     2.0   \n",
       "2      180000.0         2       1.00          770     10000     1.0   \n",
       "3      604000.0         4       3.00         1960      5000     1.0   \n",
       "4      510000.0         3       2.00         1680      8080     1.0   \n",
       "...         ...       ...        ...          ...       ...     ...   \n",
       "21592  360000.0         3       2.50         1530      1131     3.0   \n",
       "21593  400000.0         4       2.50         2310      5813     2.0   \n",
       "21594  402101.0         2       0.75         1020      1350     2.0   \n",
       "21595  400000.0         3       2.50         1600      2388     2.0   \n",
       "21596  325000.0         2       0.75         1020      1076     2.0   \n",
       "\n",
       "       waterfront  view  condition  grade  sqft_above  sqft_basement  \\\n",
       "0               0     0          3      7        1180              0   \n",
       "1               0     0          3      7        2170            400   \n",
       "2               0     0          3      6         770              0   \n",
       "3               0     0          5      7        1050            910   \n",
       "4               0     0          3      8        1680              0   \n",
       "...           ...   ...        ...    ...         ...            ...   \n",
       "21592           0     0          3      8        1530              0   \n",
       "21593           0     0          3      8        2310              0   \n",
       "21594           0     0          3      7        1020              0   \n",
       "21595           0     0          3      8        1600              0   \n",
       "21596           0     0          3      7        1020              0   \n",
       "\n",
       "       yr_built  yr_renovated      lat     long  sqft_living15  sqft_lot15  \\\n",
       "0          1955             0  47.5112 -122.257           1340        5650   \n",
       "1          1951          1991  47.7210 -122.319           1690        7639   \n",
       "2          1933             0  47.7379 -122.233           2720        8062   \n",
       "3          1965             0  47.5208 -122.393           1360        5000   \n",
       "4          1987             0  47.6168 -122.045           1800        7503   \n",
       "...         ...           ...      ...      ...            ...         ...   \n",
       "21592      2009             0  47.6993 -122.346           1530        1509   \n",
       "21593      2014             0  47.5107 -122.362           1830        7200   \n",
       "21594      2009             0  47.5944 -122.299           1020        2007   \n",
       "21595      2004             0  47.5345 -122.069           1410        1287   \n",
       "21596      2008             0  47.5941 -122.299           1020        1357   \n",
       "\n",
       "       year  month  \n",
       "0      2014     10  \n",
       "1      2014     12  \n",
       "2      2015      2  \n",
       "3      2014     12  \n",
       "4      2015      2  \n",
       "...     ...    ...  \n",
       "21592  2014      5  \n",
       "21593  2015      2  \n",
       "21594  2014      6  \n",
       "21595  2015      1  \n",
       "21596  2014     10  \n",
       "\n",
       "[21597 rows x 20 columns]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bedrooms            3.0000\n",
       "bathrooms           1.0000\n",
       "sqft_living      1180.0000\n",
       "sqft_lot         5650.0000\n",
       "floors              1.0000\n",
       "waterfront          0.0000\n",
       "view                0.0000\n",
       "condition           3.0000\n",
       "grade               7.0000\n",
       "sqft_above       1180.0000\n",
       "sqft_basement       0.0000\n",
       "yr_built         1955.0000\n",
       "yr_renovated        0.0000\n",
       "lat                47.5112\n",
       "long             -122.2570\n",
       "sqft_living15    1340.0000\n",
       "sqft_lot15       5650.0000\n",
       "year             2014.0000\n",
       "month              10.0000\n",
       "Name: 0, dtype: float64"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.drop('price', axis=1).iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_house = df.drop('price', axis=1).iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We'll scale the feature because our model was trained with scaled features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 3.00000e+00,  1.00000e+00,  1.18000e+03,  5.65000e+03,\n",
       "         1.00000e+00,  0.00000e+00,  0.00000e+00,  3.00000e+00,\n",
       "         7.00000e+00,  1.18000e+03,  0.00000e+00,  1.95500e+03,\n",
       "         0.00000e+00,  4.75112e+01, -1.22257e+02,  1.34000e+03,\n",
       "         5.65000e+03,  2.01400e+03,  1.00000e+01]])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_house.values.reshape(-1,19)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.2       , 0.08      , 0.08376422, 0.00310751, 0.        ,\n",
       "        0.        , 0.        , 0.5       , 0.4       , 0.10785619,\n",
       "        0.        , 0.47826087, 0.        , 0.57149751, 0.21760797,\n",
       "        0.16193426, 0.00582059, 0.        , 0.81818182]])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler.transform(new_house.values.reshape(-1,19))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_house = scaler.transform(new_house.values.reshape(-1,19))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict the price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "286095.84"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(new_house)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Original price:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "221900.0"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['price'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64195.84375"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abs(df['price'].iloc[0] - model.predict(new_house)[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$64195.84375 difference, not soo good, so we consider our previous options to procede, for now, we are done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
